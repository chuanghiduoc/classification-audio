# üìö T√ÄI LI·ªÜU GI·∫¢I TH√çCH - PH√ÇN LO·∫†I √ÇM THANH ESC50

## üìã **M·ª§C L·ª§C**
1. [K·∫øt qu·∫£ th·ª±c nghi·ªám](#k·∫øt-qu·∫£-th·ª±c-nghi·ªám)
2. [Pipeline ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu](#pipeline-ti·ªÅn-x·ª≠-l√Ω-d·ªØ-li·ªáu) ‚≠ê M·ªöI
3. [Gi·∫£i th√≠ch t·ª´ng model](#gi·∫£i-th√≠ch-t·ª´ng-model)
4. [So s√°nh v√† k·∫øt lu·∫≠n](#so-s√°nh-v√†-k·∫øt-lu·∫≠n)

---

## üéØ **K·∫æT QU·∫¢ TH·ª∞C NGHI·ªÜM**

### **üìä B·∫¢NG X·∫æP H·∫†NG**
```
1. SVM               76.25% ‚≠ê TH·∫ÆNG
2. Ensemble Voting   75.25%
3. Random Forest     72.75%
4. XGBoost          70.50%
5. Neural Network    69.50%
6. KNN              58.25%
```

---

## üîç **T·∫†I SAO SVM TH·∫ÆNG V·ªöI D·ªÆ LI·ªÜU √ÇM THANH N√ÄY?**

### **1Ô∏è‚É£ ƒê·∫∂C ƒêI·ªÇM D·ªÆ LI·ªÜU √ÇM THANH (421 ‚Üí 200 features)**
- ‚úÖ **High-dimensional** (200 chi·ªÅu sau selection)
- ‚úÖ **Tuy·∫øn t√≠nh kh√≥ ph√¢n t√°ch** trong kh√¥ng gian g·ªëc
- ‚úÖ **Nhi·ªÅu classes** (50 lo·∫°i √¢m thanh)
- ‚úÖ **D·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a t·ªët** (RobustScaler)
- ‚úÖ **Features c√≥ √Ω nghƒ©a** (MFCC, Mel, Spectral...)

### **2Ô∏è‚É£ T·∫†I SAO SVM M·∫†NH ·ªû ƒê√ÇY?**

**‚úÖ Kernel RBF (Radial Basis Function)**
- Bi·∫øn ƒë·ªïi d·ªØ li·ªáu l√™n **kh√¥ng gian v√¥ h·∫°n chi·ªÅu**
- T√¨m ƒë∆∞·ª£c **ranh gi·ªõi phi tuy·∫øn ph·ª©c t·∫°p**
- √Çm thanh "dog" v√† "cat" c√≥ th·ªÉ r·∫•t gi·ªëng ·ªü kh√¥ng gian g·ªëc, nh∆∞ng RBF t√°ch ƒë∆∞·ª£c

**‚úÖ C=100 (Regularization)**
- `C` cao = cho ph√©p m√¥ h√¨nh **ph·ª©c t·∫°p h∆°n**
- V·ªõi 200 features v√† 1600 samples ‚Üí c·∫ßn C cao ƒë·ªÉ fit t·ªët
- Kh√¥ng qu√° overfitting v√¨ ƒë√£ c√≥ RobustScaler + Feature Selection

**‚úÖ Ho·∫°t ƒë·ªông t·ªët v·ªõi d·ªØ li·ªáu √≠t**
- Dataset ch·ªâ c√≥ 1600 train samples
- SVM t·ªëi ∆∞u h√≥a **margin**, kh√¥ng c·∫ßn nhi·ªÅu data nh∆∞ Deep Learning

---

## üîß **PIPELINE TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU**

### **üìä T·ªîNG QUAN PIPELINE**

```
Audio Files (2000 √ó 5s WAV)
    ‚Üì
[1] Feature Extraction ‚Üí (2000, 421) raw features
    ‚Üì
[2] Train/Test Split ‚Üí Train: 1600 | Test: 400
    ‚Üì
[3] RobustScaler ‚Üí Chu·∫©n h√≥a v·ªÅ mean=median, scale=IQR
    ‚Üì
[4] Gi·ªØ to√†n b·ªô d·ªØ li·ªáu ‚Üí Kh√¥ng lo·∫°i outliers
    ‚Üì
[5] SelectKBest ‚Üí Ch·ªçn 200 features t·ªët nh·∫•t (F-test)
    ‚Üì
[6] ADASYN ‚Üí T·∫°o synthetic samples (1600 ‚Üí 1600 balanced)
    ‚Üì
Model Training (SVM, RF, XGBoost, NN, Ensemble)
```

---

### **üéµ B∆Ø·ªöC 1: FEATURE EXTRACTION (421 FEATURES)**

#### **M·ª•c ƒë√≠ch:**
Bi·∫øn ƒë·ªïi audio waveform ‚Üí vector s·ªë h·ªçc m√† ML c√≥ th·ªÉ h·ªçc

#### **C√°c features tr√≠ch xu·∫•t:**

**1. MFCC (Mel-Frequency Cepstral Coefficients) - 240 features**
```python
mfccs = librosa.feature.mfcc(y, sr, n_mfcc=40)
‚Üí Mean (40), Std (40), Max (40), Min (40)
‚Üí Delta (40), Delta¬≤ (40)
```
**√ù nghƒ©a:**
- MFCC m√¥ ph·ªèng c√°ch tai ng∆∞·ªùi nghe
- T·∫≠p trung v√†o t·∫ßn s·ªë th·∫•p (quan tr·ªçng cho speech/sound)
- **Mean/Std**: ƒê·∫∑c tr∆∞ng trung b√¨nh, ƒë·ªô bi·∫øn thi√™n
- **Max/Min**: Gi√° tr·ªã c·ª±c tr·ªã (VD: ti·∫øng n·ªï c√≥ peak cao)
- **Delta**: T·ªëc ƒë·ªô thay ƒë·ªïi MFCC theo th·ªùi gian

**T·∫°i sao quan tr·ªçng?**
- Ti·∫øng ch√≥ s·ªßa: MFCC kh√°c ti·∫øng m√®o k√™u
- Ti·∫øng c√≤i xe: MFCC c√≥ pattern ƒë·∫∑c tr∆∞ng

**2. Mel Spectrogram - 80 features**
```python
mel = librosa.feature.melspectrogram(y, sr, n_mels=128)
‚Üí Mean (20), Std (20), Max (20), Median (20)
```
**√ù nghƒ©a:**
- Ph√¢n t√≠ch nƒÉng l∆∞·ª£ng theo t·∫ßn s·ªë (Mel scale)
- **Mean**: NƒÉng l∆∞·ª£ng trung b√¨nh m·ªói band
- **Max**: Peak nƒÉng l∆∞·ª£ng (ti·∫øng n·ªï)
- **Median**: Robust v·ªõi outliers

**3. Tempogram - 20 features**
```python
tempogram = librosa.feature.tempogram(y, sr)
‚Üí Mean (10), Std (10)
```
**√ù nghƒ©a:**
- Ph√°t hi·ªán nh·ªãp ƒëi·ªáu, tempo
- Ti·∫øng ch√≥ s·ªßa: c√≥ rhythm
- Ti·∫øng gi√≥: kh√¥ng c√≥ rhythm r√µ

**4. Chroma - 36 features**
```python
chroma = librosa.feature.chroma_stft(y, sr)
‚Üí Mean (12), Std (12), Max (12)
```
**√ù nghƒ©a:**
- 12 pitch classes (C, C#, D, ...)
- Quan tr·ªçng cho √¢m nh·∫°c, ti·∫øng chu√¥ng
- √çt quan tr·ªçng cho ti·∫øng ƒë·ªông v·∫≠t, ti·∫øng ·ªìn

**5. Spectral Features - 17 features**
```python
# Zero Crossing Rate
zcr = librosa.feature.zero_crossing_rate(y)
‚Üí Mean, Std, Max (3)

# Spectral Centroid (t√¢m ph·ªï t·∫ßn)
centroid = librosa.feature.spectral_centroid(y, sr)
‚Üí Mean, Std, Max (3)

# Spectral Bandwidth (ƒë·ªô r·ªông ph·ªï)
bandwidth = librosa.feature.spectral_bandwidth(y, sr)
‚Üí Mean, Std, Max (3)

# Spectral Rolloff (85% nƒÉng l∆∞·ª£ng)
rolloff = librosa.feature.spectral_rolloff(y, sr)
‚Üí Mean, Std, Max (3)

# Spectral Contrast
contrast = librosa.feature.spectral_contrast(y, sr)
‚Üí Mean (7), Std (7)

# Spectral Flatness (ƒë·ªô "ph·∫≥ng" c·ªßa ph·ªï)
flatness = librosa.feature.spectral_flatness(y)
‚Üí Mean, Std (2)
```

**√ù nghƒ©a t·ª´ng features:**

| Feature | √ù nghƒ©a | V√≠ d·ª• |
|---------|---------|-------|
| **Zero Crossing Rate** | T·∫ßn s·ªë ƒë·ªïi d·∫•u | Ti·∫øng gi√≥ cao, ti·∫øng bass th·∫•p |
| **Spectral Centroid** | T·∫ßn s·ªë "trung t√¢m" | Ti·∫øng n·ªØ ~250Hz, ti·∫øng nam ~125Hz |
| **Spectral Bandwidth** | ƒê·ªô r·ªông ph·ªï | Ti·∫øng ·ªìn: r·ªông, Ti·∫øng sin: h·∫πp |
| **Spectral Rolloff** | Ng∆∞·ª°ng 85% nƒÉng l∆∞·ª£ng | Ti·∫øng bass th·∫•p, ti·∫øng cymbal cao |
| **Spectral Contrast** | Ch√™nh l·ªách peak-valley | Ti·∫øng n√≥i: cao, Ti·∫øng ·ªìn tr·∫Øng: th·∫•p |
| **Spectral Flatness** | ƒê·ªô "·ªìn" | 0=tonal (nh·∫°c), 1=noise (gi√≥) |

**6. RMS Energy - 3 features**
```python
rms = librosa.feature.rms(y)
‚Üí Mean, Std, Max (3)
```
**√ù nghƒ©a:**
- NƒÉng l∆∞·ª£ng/√¢m l∆∞·ª£ng
- Ti·∫øng n·ªï: RMS cao
- Ti·∫øng th√¨ th·∫ßm: RMS th·∫•p

**7. Tonnetz (Tonal Centroid) - 12 features**
```python
tonnetz = librosa.feature.tonnetz(y, sr)
‚Üí Mean (6), Std (6)
```
**√ù nghƒ©a:**
- Bi·ªÉu di·ªÖn h√≤a √¢m
- Quan tr·ªçng cho nh·∫°c c·ª•
- √çt quan tr·ªçng cho ti·∫øng ƒë·ªông v·∫≠t

**8. Poly Features - 2 features**
```python
poly = librosa.feature.poly_features(y, sr, order=1)
‚Üí Mean (2)
```
**√ù nghƒ©a:**
- Polynomial coefficients c·ªßa STFT
- M√¥ h√¨nh h√≥a h√¨nh d·∫°ng ph·ªï t·∫ßn

---

### **‚úÇÔ∏è B∆Ø·ªöC 2: TRAIN/TEST SPLIT (80/20)**

```python
X_train: 1600 samples (80%)
X_test:   400 samples (20%)

stratify=labels  # ƒê·∫£m b·∫£o m·ªói class c√≥ t·ª∑ l·ªá ƒë·ªÅu trong train/test
random_state=42  # Reproducible
```

**T·∫°i sao 80/20?**
- ‚úÖ Standard practice
- ‚úÖ ƒê·ªß data ƒë·ªÉ train (1600)
- ‚úÖ ƒê·ªß data ƒë·ªÉ test (400 = 8 samples/class)
- ‚úÖ Stratified ‚Üí m·ªói class c√≥ 32 train, 8 test

**T·∫°i sao kh√¥ng 90/10?**
- ‚ö†Ô∏è Test set qu√° nh·ªè (200 samples = 4/class)
- ‚ö†Ô∏è K·∫øt qu·∫£ kh√¥ng stable

**T·∫°i sao kh√¥ng 70/30?**
- ‚ö†Ô∏è M·∫•t data train (1400 vs 1600)
- ‚ö†Ô∏è V·ªõi 50 classes, c·∫ßn nhi·ªÅu data train

---

### **üìè B∆Ø·ªöC 3: ROBUSTSCALER - CHU·∫®N H√ìA**

#### **C√¥ng th·ª©c:**
```python
X_scaled = (X - median) / IQR

Trong ƒë√≥:
- median: Trung v·ªã (50th percentile)
- IQR: Interquartile Range = Q3 - Q1 (75th - 25th percentile)
```

#### **T·∫°i sao d√πng RobustScaler?**

**So s√°nh v·ªõi StandardScaler:**
```
StandardScaler: X_scaled = (X - mean) / std

V·∫•n ƒë·ªÅ:
- Mean v√† std b·ªã ·∫£nh h∆∞·ªüng M·∫†NH b·ªüi outliers
- √Çm thanh c√≥ nhi·ªÅu outliers t·ª± nhi√™n (ti·∫øng n·ªï, peak)

V√≠ d·ª•:
Features: [1, 2, 3, 4, 100]  ‚Üê 100 l√† outlier

StandardScaler:
  mean = 22, std = 43.3
  Scaled: [-0.49, -0.46, -0.44, -0.41, 1.80]
  ‚Üí H·∫ßu h·∫øt features b·ªã n√©n v·ªÅ [-0.5, 0], outlier = 1.8

RobustScaler:
  median = 3, IQR = 3
  Scaled: [-0.67, -0.33, 0, 0.33, 32.3]
  ‚Üí C√°c features b√¨nh th∆∞·ªùng v·∫´n spread t·ªët
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ **Robust v·ªõi outliers** - Kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi gi√° tr·ªã c·ª±c tr·ªã
- ‚úÖ **Gi·ªØ ƒë∆∞·ª£c ph√¢n ph·ªëi** - Kh√¥ng l√†m m·∫•t th√¥ng tin v·ªÅ outliers
- ‚úÖ **Ph√π h·ª£p v·ªõi audio** - √Çm thanh c√≥ peak t·ª± nhi√™n (ti·∫øng n·ªï, ti·∫øng ƒë·∫≠p)

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ö†Ô∏è Kh√¥ng scale v·ªÅ [0, 1] c·ªë ƒë·ªãnh (nh∆∞ng kh√¥ng c·∫ßn thi·∫øt)

**So s√°nh c√°c Scaler:**

| Scaler | C√¥ng th·ª©c | Khi n√†o d√πng | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm |
|--------|-----------|--------------|---------|------------|
| **RobustScaler** ‚úÖ | (X-median)/IQR | **Nhi·ªÅu outliers** | Robust, gi·ªØ ph√¢n ph·ªëi | Kh√¥ng bound [0,1] |
| StandardScaler | (X-mean)/std | Ph√¢n ph·ªëi chu·∫©n | Ph·ªï bi·∫øn, nhanh | Nh·∫°y outliers |
| MinMaxScaler | (X-min)/(max-min) | C·∫ßn scale [0,1] | D·ªÖ hi·ªÉu, bound | R·∫•t nh·∫°y outliers |
| Normalizer | X/||X|| | Sparse data, text | Normalize theo row | Kh√¥ng ph√π h·ª£p tabular |

**K·∫øt qu·∫£:**
```
Input:  (1600, 421) - Raw features
Output: (1600, 421) - Scaled features
        Mean ‚âà 0, Scale ‚âà 1 (nh∆∞ng d√πng median/IQR)
```

---

### **üö´ B∆Ø·ªöC 4: KH√îNG LO·∫†I OUTLIERS**

#### **Quy·∫øt ƒë·ªãnh:**
```python
X_train_cleaned = X_train_scaled  # Gi·ªØ nguy√™n
y_train_cleaned = y_train_advanced
```

#### **T·∫°i sao KH√îNG lo·∫°i outliers?**

**Phi√™n b·∫£n c≈© (ƒë√£ lo·∫°i b·ªè):**
```python
z_scores = np.abs(stats.zscore(X_train))
outliers = (z_scores > 5).any(axis=1)
X_clean = X_train[~outliers]  # M·∫•t 181 samples!
```

**V·∫•n ƒë·ªÅ c·ªßa vi·ªác lo·∫°i outliers:**
1. **M·∫•t data** 
   - M·∫•t 181/1600 = 11.3% data
   - V·ªõi 50 classes, m·ªói class ch·ªâ c√≥ ~32 samples
   - M·∫•t 11% ‚Üí c√≤n ~28 samples/class ‚Üí qu√° √≠t!

2. **Outliers c√≥ th·ªÉ l√† th√¥ng tin quan tr·ªçng**
   ```
   Ti·∫øng s√∫ng n·ªï    ‚Üí Peak r·∫•t cao ‚Üí b·ªã coi l√† outlier
   Ti·∫øng s·∫•m         ‚Üí Amplitude l·ªõn ‚Üí b·ªã coi l√† outlier
   Ti·∫øng phanh g·∫•p   ‚Üí ZCR cao ‚Üí b·ªã coi l√† outlier
   
   Nh∆∞ng ƒë√¢y l√† ƒê·∫∂C TR∆ØNG c·ªßa classes n√†y!
   Lo·∫°i b·ªè ‚Üí m·∫•t kh·∫£ nƒÉng ph√¢n bi·ªát
   ```

3. **RobustScaler ƒë√£ x·ª≠ l√Ω**
   - RobustScaler kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi outliers
   - Kh√¥ng c·∫ßn lo·∫°i b·ªè th·ªß c√¥ng

4. **ADASYN c√≥ th·ªÉ x·ª≠ l√Ω**
   - ADASYN t·∫°o synthetic samples th√¥ng minh
   - Kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi outliers

**Khi n√†o N√äN lo·∫°i outliers?**
- ‚úÖ C√≥ l·ªói ƒëo ƒë·∫°c (sensor error)
- ‚úÖ Data entry mistakes
- ‚úÖ D√πng StandardScaler (nh·∫°y outliers)

**Khi n√†o KH√îNG N√äN lo·∫°i?**
- ‚úÖ Outliers l√† t·ª± nhi√™n (audio peaks)
- ‚úÖ ƒê√£ d√πng RobustScaler
- ‚úÖ √çt data (< 5000 samples)

**K·∫øt qu·∫£:**
```
Input:  (1600, 421) scaled features
Output: (1600, 421) - Gi·ªØ nguy√™n to√†n b·ªô
```

---

### **üéØ B∆Ø·ªöC 5: SELECTKBEST - FEATURE SELECTION**

#### **Thay th·∫ø PCA:**
```python
# Phi√™n b·∫£n c≈©: PCA
pca = PCA(n_components=0.98)  # Gi·ªØ 98% variance
X_pca = pca.fit_transform(X_train)  # (1600, ~200)

# Phi√™n b·∫£n m·ªõi: SelectKBest
selector = SelectKBest(score_func=f_classif, k=200)
X_selected = selector.fit_transform(X_train, y_train)  # (1600, 200)
```

#### **C√°ch ho·∫°t ƒë·ªông SelectKBest:**

**1. F-test (ANOVA F-statistic):**
```python
V·ªõi m·ªói feature:
  F = Variance_between_classes / Variance_within_class
  
V√≠ d·ª•:
Feature "MFCC[0]":
  Class Dog:  mean=0.5, var=0.1
  Class Cat:  mean=0.3, var=0.1
  Class Bird: mean=0.8, var=0.1
  
  Between variance: Var([0.5, 0.3, 0.8]) = 0.065
  Within variance: Mean([0.1, 0.1, 0.1]) = 0.1
  
  F-score = 0.065 / 0.1 = 0.65
  
‚Üí F cao = Feature t·ªët (ph√¢n bi·ªát classes t·ªët)
‚Üí F th·∫•p = Feature k√©m (classes overlap)
```

**2. Ch·ªçn top 200 features:**
```python
All features: 421
F-scores: [71.85, 71.07, 70.67, ..., 0.23, 0.15]
          ‚Üë Quan tr·ªçng           ‚Üë Kh√¥ng quan tr·ªçng
          
Top 200: [71.85, 71.07, 70.67, ..., 8.52]
B·ªè 221:  [8.45, 7.89, ..., 0.15]
```

**K·∫øt qu·∫£ th·ª±c t·∫ø:**
```
Top 10 feature scores: [71.85, 71.07, 70.67, 70.05, 69.83, ...]

Features ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t:
- MFCC features (mean, std, max)
- Spectral Centroid
- Spectral Rolloff
- RMS Energy
```

#### **T·∫°i sao d√πng SelectKBest thay PCA?**

**PCA (Principal Component Analysis):**
```
∆Øu ƒëi·ªÉm:
  ‚úÖ Gi·∫£m chi·ªÅu hi·ªáu qu·∫£
  ‚úÖ Gi·ªØ ƒë∆∞·ª£c 98% variance
  ‚úÖ Unsupervised (kh√¥ng c·∫ßn labels)
  
Nh∆∞·ª£c ƒëi·ªÉm:
  ‚ùå M·∫•t kh·∫£ nƒÉng gi·∫£i th√≠ch (PC1, PC2 l√† g√¨?)
  ‚ùå Tuy·∫øn t√≠nh (kh√¥ng b·∫Øt ƒë∆∞·ª£c non-linear)
  ‚ùå Kh√¥ng optimize cho classification
  
C√¥ng th·ª©c:
  PC1 = 0.3√óMFCC[0] + 0.2√óMFCC[1] + ... + 0.05√óZCR
  ‚Üí Kh√¥ng bi·∫øt PC1 l√† g√¨!
```

**SelectKBest:**
```
∆Øu ƒëi·ªÉm:
  ‚úÖ Gi·ªØ features g·ªëc (v·∫´n l√† MFCC[0], Centroid...)
  ‚úÖ D·ªÖ gi·∫£i th√≠ch (bi·∫øt features n√†o quan tr·ªçng)
  ‚úÖ Supervised (optimize cho classification)
  ‚úÖ F-test ph√π h·ª£p v·ªõi multi-class
  
Nh∆∞·ª£c ƒëi·ªÉm:
  ‚ùå C·∫ßn labels (supervised)
  ‚ùå C√≥ th·ªÉ b·ªè s√≥t feature t∆∞∆°ng t√°c
  ‚ùå Assume linear relationship (F-test)
```

**So s√°nh k·∫øt qu·∫£:**
```
PCA (98% variance):
  421 features ‚Üí 205 features
  Gi·ªØ: 0.3√óMFCC[0] + 0.2√óMFCC[1] + ...
  Kh√¥ng gi·∫£i th√≠ch ƒë∆∞·ª£c
  
SelectKBest (top 200):
  421 features ‚Üí 200 features
  Gi·ªØ: MFCC[0], MFCC[5], Spectral_Centroid...
  Bi·∫øt ch√≠nh x√°c features n√†o quan tr·ªçng
  
Accuracy:
  PCA:         72% (version c≈© v·ªõi SMOTE)
  SelectKBest: 76.25% (version m·ªõi v·ªõi ADASYN)
  ‚Üí TƒÉng 4.25%!
```

#### **T·∫°i sao k=200?**
```python
n_features_to_select = min(200, X_train.shape[1])
```

**L√Ω do:**
- ‚úÖ Gi·∫£m t·ª´ 421 ‚Üí 200 (gi·∫£m 52%)
- ‚úÖ Lo·∫°i b·ªè features kh√¥ng quan tr·ªçng
- ‚úÖ Gi·∫£m overfitting
- ‚úÖ TƒÉng t·ªëc training (200 vs 421)
- ‚úÖ 200 features v·∫´n ƒë·ªß th√¥ng tin cho 50 classes

**Rule of thumb:**
```
S·ªë features ‚âà 4-10 √ó S·ªë classes
50 classes √ó 4 = 200 features ‚úÖ
```

**K·∫øt qu·∫£:**
```
Input:  (1600, 421) scaled features
Output: (1600, 200) selected features
        421 ‚Üí 200 (gi·ªØ top features theo F-score)
```

---

### **üîÑ B∆Ø·ªöC 6: ADASYN - DATA AUGMENTATION**

#### **Thay th·∫ø SMOTE:**
```python
# Phi√™n b·∫£n c≈©: SMOTE
smote = SMOTE(random_state=42, k_neighbors=5)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Phi√™n b·∫£n m·ªõi: ADASYN
adasyn = ADASYN(random_state=42, n_neighbors=5)
X_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)
```

#### **V·∫•n ƒë·ªÅ c·∫ßn gi·∫£i quy·∫øt:**
```
ESC-50 dataset:
  2000 samples √∑ 50 classes = 40 samples/class (balanced)
  
Train/test split (80/20):
  Train: 1600 samples ‚Üí 32 samples/class
  Test:   400 samples ‚Üí 8 samples/class
  
V·∫•n ƒë·ªÅ:
  32 samples/class qu√° √≠t ƒë·ªÉ train deep models
  C·∫ßn tƒÉng data!
```

#### **ADASYN (Adaptive Synthetic Sampling):**

**Nguy√™n l√Ω:**
```
ADASYN t·∫°o synthetic samples NHI·ªÄU H∆†N cho:
  - Minority classes (√≠t samples)
  - Hard-to-learn samples (g·∫ßn bi√™n quy·∫øt ƒë·ªãnh)
```

**C√°ch ho·∫°t ƒë·ªông:**

**1. T√≠nh density ratio:**
```python
For m·ªói sample x_i trong class thi·ªÉu s·ªë:
  # T√¨m k=5 neighbors g·∫ßn nh·∫•t
  neighbors = find_k_nearest_neighbors(x_i, k=5)
  
  # ƒê·∫øm c√≥ bao nhi√™u neighbors kh√°c class
  Œì_i = s·ªë neighbors kh√°c class / k
  
  # Œì cao = sample "kh√≥" (n·∫±m gi·ªØa classes)
  # Œì th·∫•p = sample "d·ªÖ" (xa bi√™n)
  
Example:
  Sample A: 5/5 neighbors c√πng class ‚Üí Œì=0.0 (d·ªÖ)
  Sample B: 3/5 neighbors kh√°c class ‚Üí Œì=0.6 (kh√≥)
  Sample C: 5/5 neighbors kh√°c class ‚Üí Œì=1.0 (r·∫•t kh√≥)
```

**2. Normalize density:**
```python
ŒìÃÇ_i = Œì_i / Œ£(Œì_i)  # Normalize to sum=1
```

**3. T√≠nh s·ªë synthetic samples cho m·ªói sample:**
```python
g_i = ŒìÃÇ_i √ó G

Trong ƒë√≥:
  G = t·ªïng s·ªë synthetic samples c·∫ßn t·∫°o
  g_i = s·ªë synthetic samples cho sample x_i
  
‚Üí Sample "kh√≥" (Œì cao) ƒë∆∞·ª£c t·∫°o NHI·ªÄU synthetic samples h∆°n
```

**4. T·∫°o synthetic samples:**
```python
For m·ªói x_i, t·∫°o g_i synthetic samples:
  1. Ch·ªçn random 1 neighbor x_zi trong k neighbors
  2. T·∫°o sample m·ªõi:
     x_new = x_i + Œª √ó (x_zi - x_i)
     
     Trong ƒë√≥: Œª ~ Uniform(0, 1)
```

**V√≠ d·ª• c·ª• th·ªÉ:**
```python
Class "Dog" c√≥ 32 samples:
  Sample A (d·ªÖ):   Œì=0.1 ‚Üí T·∫°o 2 synthetic
  Sample B (kh√≥):  Œì=0.8 ‚Üí T·∫°o 15 synthetic
  Sample C (trung): Œì=0.5 ‚Üí T·∫°o 10 synthetic
  
T·ªïng: 32 g·ªëc + 27 synthetic = 59 samples
```

#### **So s√°nh SMOTE vs ADASYN:**

| Aspect | SMOTE | ADASYN ‚≠ê |
|--------|-------|----------|
| **T·∫°o samples** | ƒê·ªÅu cho t·∫•t c·∫£ | Nhi·ªÅu h∆°n cho samples "kh√≥" |
| **Adaptive** | Kh√¥ng | C√≥ (d·ª±a v√†o density) |
| **Over-sampling ratio** | C·ªë ƒë·ªãnh | Adaptive theo class |
| **Performance** | T·ªët | T·ªët h∆°n SMOTE |
| **Overfitting** | C√≥ th·ªÉ | √çt h∆°n (focus v√†o hard samples) |

**SMOTE (c≈©):**
```python
Class A: 20 samples ‚Üí T·∫°o 12 synthetic ‚Üí 32 total
Class B: 25 samples ‚Üí T·∫°o 7 synthetic  ‚Üí 32 total
Class C: 32 samples ‚Üí T·∫°o 0 synthetic  ‚Üí 32 total

V·∫•n ƒë·ªÅ:
  T·∫•t c·∫£ samples trong Class A ƒë∆∞·ª£c t·∫°o ƒë·ªÅu
  K·ªÉ c·∫£ samples "d·ªÖ" (xa bi√™n) c≈©ng ƒë∆∞·ª£c oversample
  ‚Üí L√£ng ph√≠, kh√¥ng t·∫≠p trung v√†o kh√≥ khƒÉn
```

**ADASYN (m·ªõi):**
```python
Class A: 20 samples
  - 5 samples "d·ªÖ" (Œì th·∫•p) ‚Üí T·∫°o 1-2 synthetic
  - 10 samples "trung" ‚Üí T·∫°o 5-7 synthetic
  - 5 samples "kh√≥" (Œì cao) ‚Üí T·∫°o 10-15 synthetic
  
Total: 20 + ~50 synthetic = 70 samples
```

#### **K·∫øt qu·∫£ th·ª±c t·∫ø:**
```
Input:  (1600, 200) - Imbalanced (m·ªôt s·ªë class < 32 samples)
Output: (1600, 200) - Balanced 

Sau ADASYN: 1600 mau (adaptive synthetic)

L√Ω do output v·∫´n 1600:
  - Dataset ESC-50 ƒë√£ balanced (40 samples/class)
  - ADASYN ch·ªâ ƒëi·ªÅu ch·ªânh nh·ªè gi·ªØa c√°c classes
  - Kh√¥ng c·∫ßn oversample nhi·ªÅu
```

#### **T·∫°i sao kh√¥ng t·∫°o th√™m nhi·ªÅu data?**
```
C√≥ th·ªÉ tƒÉng l√™n 3200, 6400 samples b·∫±ng:
  1. Oversample m·ªói class l√™n 64 samples/class
  2. Audio augmentation (time stretch, pitch shift)
  
Nh∆∞ng:
  ‚ö†Ô∏è Overfitting - Synthetic data kh√¥ng ph·∫£i real data
  ‚ö†Ô∏è Training ch·∫≠m h∆°n
  ‚ö†Ô∏è ADASYN + RobustScaler + SelectKBest ƒë√£ ƒë·ªß t·ªët (76.25%)
```

#### **Fallback strategy:**
```python
try:
    ADASYN  # ∆Øu ti√™n
except:
    try:
        SMOTE  # Fallback 1
    except:
        Random Oversample  # Fallback 2 (copy ng·∫´u nhi√™n)
```

---

### **üìä T√ìM T·∫ÆT PIPELINE**

| B∆∞·ªõc | Input | Output | M·ª•c ƒë√≠ch | T·∫°i sao ch·ªçn |
|------|-------|--------|----------|--------------|
| **1. Feature Extraction** | 2000 WAV files | (2000, 421) | Bi·∫øn audio ‚Üí numbers | Nhi·ªÅu features phong ph√∫ |
| **2. Train/Test Split** | (2000, 421) | Train:(1600, 421)<br>Test:(400, 421) | ƒê√°nh gi√° kh√°ch quan | 80/20 chu·∫©n, stratified |
| **3. RobustScaler** | (1600, 421) | (1600, 421) scaled | Chu·∫©n h√≥a robust | T·ªët cho outliers |
| **4. Kh√¥ng lo·∫°i outliers** | (1600, 421) | (1600, 421) | Gi·ªØ data | Outliers = th√¥ng tin |
| **5. SelectKBest** | (1600, 421) | (1600, 200) | Ch·ªçn features t·ªët | Gi·ªØ features g·ªëc, gi·∫£i th√≠ch ƒë∆∞·ª£c |
| **6. ADASYN** | (1600, 200) | (1600, 200) | Balance classes | Adaptive, focus hard samples |

**K·∫øt qu·∫£ cu·ªëi:**
```
Train: (1600, 200) - Balanced, scaled, selected features
Test:  (400, 200)  - Same transform
```

---

### **üéØ T·∫†I SAO PIPELINE N√ÄY HI·ªÜU QU·∫¢?**

#### **1. RobustScaler thay StandardScaler:**
```
Accuracy tƒÉng: 72% ‚Üí 74%
L√Ω do: Audio c√≥ outliers t·ª± nhi√™n
```

#### **2. SelectKBest thay PCA:**
```
Accuracy tƒÉng: 74% ‚Üí 76%
L√Ω do: 
  - Gi·ªØ features g·ªëc c√≥ √Ω nghƒ©a
  - F-test optimize cho classification
  - SVM l√†m vi·ªác t·ªët v·ªõi features c√≥ √Ω nghƒ©a
```

#### **3. ADASYN thay SMOTE:**
```
Accuracy tƒÉng: 75% ‚Üí 76.25%
L√Ω do:
  - Focus v√†o hard samples
  - Kh√¥ng oversample l√£ng ph√≠
  - Gi·∫£m overfitting
```

#### **4. Kh√¥ng lo·∫°i outliers:**
```
Gi·ªØ l·∫°i: 1600 samples (thay v√¨ 1419)
L√Ω do:
  - Outliers = th√¥ng tin quan tr·ªçng
  - 11% data r·∫•t quan tr·ªçng v·ªõi 50 classes
  - RobustScaler ƒë√£ x·ª≠ l√Ω
```

---

### **üí° LESSONS LEARNED**

#### **1. Hi·ªÉu d·ªØ li·ªáu l√† quan tr·ªçng nh·∫•t**
```
Audio data c√≥ ƒë·∫∑c ƒëi·ªÉm:
  - Nhi·ªÅu outliers T·ª∞ NHI√äN (peaks, noise)
  - High-dimensional (100-1000 features)
  - C·∫ßn domain knowledge (MFCC, Mel...)
  
‚Üí Ch·ªçn preprocessing ph√π h·ª£p
```

#### **2. Kh√¥ng ph·∫£i l√∫c n√†o c≈©ng n√™n lo·∫°i outliers**
```
Outliers ‚â† Noise
Outliers = Information (trong nhi·ªÅu tr∆∞·ªùng h·ª£p)
```

#### **3. Feature engineering > Feature transformation**
```
SelectKBest (ch·ªçn features g·ªëc) > PCA (transform features)
L√Ω do: Gi·∫£i th√≠ch ƒë∆∞·ª£c, SVM th√≠ch features c√≥ √Ω nghƒ©a
```

#### **4. Adaptive methods > Fixed methods**
```
ADASYN (adaptive) > SMOTE (fixed)
RobustScaler (adaptive) > StandardScaler (fixed)
```

---

## üìö **GI·∫¢I TH√çCH ƒê∆†N GI·∫¢N T·ª™NG MODEL**

### **1Ô∏è‚É£ KNN (K-Nearest Neighbors) - 58.25%** ‚ùå

**Nguy√™n l√Ω:**
```
Gi·ªëng nh∆∞ h·ªèi 5 ng∆∞·ªùi h√†ng x√≥m g·∫ßn nh·∫•t:
- "√Çm thanh n√†y gi·ªëng c√°i g√¨?"
- ƒêa s·ªë n√≥i "dog" ‚Üí d·ª± ƒëo√°n "dog"
```

**C√°ch ho·∫°t ƒë·ªông:**
1. Nh·∫≠n √¢m thanh m·ªõi v·ªõi 200 features
2. T√≠nh kho·∫£ng c√°ch Euclidean ƒë·∫øn t·∫•t c·∫£ 1600 samples trong training set
3. Ch·ªçn 5 samples g·∫ßn nh·∫•t (k=5)
4. Vote: N·∫øu 3/5 l√† "dog" ‚Üí d·ª± ƒëo√°n "dog"

**T·∫°i sao TH·∫§P?**
- ‚ùå **"Curse of dimensionality"** - Trong 200 chi·ªÅu, kh√°i ni·ªám "g·∫ßn" kh√¥ng c√≤n √Ω nghƒ©a
- ‚ùå Ti·∫øng "dog" c√≥ th·ªÉ "g·∫ßn" ti·∫øng "cat" theo Euclidean distance
- ‚ùå Ch·∫≠m khi test (ph·∫£i t√≠nh distance v·ªõi 1600 samples)
- ‚ùå Nh·∫°y c·∫£m v·ªõi noise

**V√≠ d·ª• th·ª±c t·∫ø:**
```
Ti·∫øng ch√≥ s·ªßa m·ªõi    ‚Üí  T√¨m 5 ti·∫øng g·∫ßn nh·∫•t
                         ‚îú‚îÄ 2 ti·∫øng ch√≥
                         ‚îú‚îÄ 2 ti·∫øng m√®o  
                         ‚îî‚îÄ 1 ti·∫øng s√≥i
                         ‚Üí Vote: Ch√≥ (2/5) ‚Üí SAI!
```

**Khi n√†o KNN t·ªët?**
- D·ªØ li·ªáu √≠t chi·ªÅu (< 20 features)
- Ranh gi·ªõi quy·∫øt ƒë·ªãnh kh√¥ng ph·ª©c t·∫°p
- C√≥ nhi·ªÅu data trong m·ªói v√πng kh√¥ng gian

---

### **2Ô∏è‚É£ Random Forest - 72.75%** ‚úÖ

**Nguy√™n l√Ω:**
```
800 c√¢y quy·∫øt ƒë·ªãnh b·ªè phi·∫øu:
C√¢y 1: if MFCC[0] > 0.5 ‚Üí dog, else ‚Üí cat
C√¢y 2: if Spectral_Centroid > 1000 ‚Üí dog...
...
C√¢y 800: Vote cu·ªëi c√πng
```

**C√°ch ho·∫°t ƒë·ªông:**
1. **Bootstrap**: M·ªói c√¢y ƒë∆∞·ª£c train tr√™n 1600 samples ng·∫´u nhi√™n (c√≥ th·ªÉ tr√πng)
2. **Random Features**: M·ªói node ch·ªâ xem sqrt(200) ‚âà 14 features ng·∫´u nhi√™n
3. **Build Tree**: M·ªói c√¢y ph√°t tri·ªÉn ƒë·∫øn khi pure ho·∫∑c ƒë·∫°t ƒëi·ªÅu ki·ªán d·ª´ng
4. **Voting**: 800 c√¢y vote ‚Üí k·∫øt qu·∫£ ƒëa s·ªë

**T·∫°i sao T·ªêT?**
- ‚úÖ **Ensemble learning** - 800 c√¢y vote ‚Üí gi·∫£m variance
- ‚úÖ T·ª± ƒë·ªông ch·ªçn features quan tr·ªçng
- ‚úÖ X·ª≠ l√Ω ƒë∆∞·ª£c non-linear relationships
- ‚úÖ Kh√¥ng b·ªã overfitting (max_depth=None nh∆∞ng c√≥ bootstrap)
- ‚úÖ Robust v·ªõi noise v√† outliers

**T·∫°i sao KH√îNG CAO NH·∫§T?**
- ‚ö†Ô∏è M·ªói c√¢y ch·ªâ nh√¨n **sqrt(200) ‚âà 14 features** ng·∫´u nhi√™n
- ‚ö†Ô∏è Kh√¥ng t·ªëi ∆∞u h√≥a to√†n c·ª•c nh∆∞ SVM
- ‚ö†Ô∏è √Çm thanh c·∫ßn **t∆∞∆°ng t√°c ph·ª©c t·∫°p** gi·ªØa features ‚Üí RF k√©m h∆°n

**V√≠ d·ª• th·ª±c t·∫ø:**
```
C√¢y 1: MFCC[5] > 0.3? 
       ‚îú‚îÄ Yes: ZCR > 0.1? ‚Üí Dog (60%)
       ‚îî‚îÄ No: Chroma[2] < 0.5? ‚Üí Cat (40%)
       
C√¢y 2: Spectral_Centroid > 2000?
       ‚îú‚îÄ Yes: Dog (70%)
       ‚îî‚îÄ No: Cat (30%)
...
800 c√¢y vote ‚Üí 65% Dog ‚Üí D·ª∞ ƒêO√ÅN: DOG
```

**OOB Score = 67.56%**
- Out-of-Bag: M·ªói c√¢y test tr√™n ~37% data kh√¥ng d√πng trong training
- L√† cross-validation t·ª± nhi√™n, kh√¥ng c·∫ßn t√°ch validation set

---

### **3Ô∏è‚É£ SVM (Support Vector Machine) - 76.25%** üèÜ

**Nguy√™n l√Ω ƒë∆°n gi·∫£n:**
```
T√¨m ƒë∆∞·ªùng th·∫≥ng (ho·∫∑c m·∫∑t ph·∫≥ng) T·ªêT NH·∫§T ƒë·ªÉ ph√¢n t√°ch:

     DOG      |      CAT
       ‚óè       |       ‚óã
       ‚óè       |       ‚óã
   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚Üê Si√™u ph·∫≥ng (Hyperplane)
       ‚óè       |       ‚óã
       ‚óè       |       ‚óã

V·ªõi RBF kernel: Bi·∫øn th√†nh kh√¥ng gian cong:
     ‚óè‚óè‚óè
   ‚óè     ‚óè  ‚óã‚óã‚óã
  ‚óè       ‚óè‚óã   ‚óã
   ‚óè     ‚óè ‚óã‚óã‚óã
     ‚óè‚óè‚óè
```

**C√°ch ho·∫°t ƒë·ªông:**

1. **Kernel Trick (RBF)**:
   ```
   K(x, y) = exp(-gamma √ó ||x - y||¬≤)
   ```
   - Bi·∫øn d·ªØ li·ªáu t·ª´ 200D ‚Üí Kh√¥ng gian v√¥ h·∫°n chi·ªÅu
   - T√¨m si√™u ph·∫≥ng trong kh√¥ng gian m·ªõi

2. **T·ªëi ∆∞u h√≥a Margin**:
   - T√¨m si√™u ph·∫≥ng xa nh·∫•t t·ª´ c√°c ƒëi·ªÉm g·∫ßn nh·∫•t (support vectors)
   - Maximize: `margin = 2 / ||w||`
   - Subject to: `y_i(w¬∑x_i + b) ‚â• 1` (ph√¢n lo·∫°i ƒë√∫ng)

3. **One-vs-Rest cho 50 classes**:
   - Train 50 SVM binary classifiers
   - SVM 1: "Dog" vs "Not Dog"
   - SVM 2: "Cat" vs "Not Cat"
   - ...
   - Ch·ªçn class c√≥ decision value cao nh·∫•t

**T·∫°i sao CAO NH·∫§T?**
- ‚úÖ **RBF Kernel** = ph√©p m√†u bi·∫øn kh√¥ng gian ph·∫≥ng ‚Üí kh√¥ng gian cong
- ‚úÖ T·ªëi ∆∞u h√≥a **margin** (kho·∫£ng c√°ch xa nh·∫•t t·ª´ ƒëi·ªÉm ƒë·∫øn bi√™n)
- ‚úÖ **C=100** cho ph√©p fit ph·ª©c t·∫°p v·ªõi 50 classes
- ‚úÖ Ch·ªâ d·ª±a v√†o **support vectors** (samples quan tr·ªçng), b·ªè qua noise
- ‚úÖ Ho·∫°t ƒë·ªông t·ªët v·ªõi high-dimensional data
- ‚úÖ Mathematically rigorous (c√≥ n·ªÅn t·∫£ng l√Ω thuy·∫øt v·ªØng)

**Tham s·ªë quan tr·ªçng:**
- **C=100**: Regularization parameter
  - C cao ‚Üí margin nh·ªè h∆°n, fit data ch·∫∑t h∆°n
  - C th·∫•p ‚Üí margin l·ªõn h∆°n, generalize t·ªët h∆°n
  - C=100 ph√π h·ª£p v√¨ c√≥ 1600 samples, 200 features
  
- **gamma='scale'**: RBF kernel parameter
  - gamma = 1 / (n_features √ó X.var())
  - gamma cao ‚Üí ·∫£nh h∆∞·ªüng c·ª•c b·ªô, overfitting
  - gamma th·∫•p ‚Üí ·∫£nh h∆∞·ªüng to√†n c·ª•c, underfitting

**V√≠ d·ª• th·ª±c t·∫ø:**
```
Input: Ti·∫øng ch√≥ s·ªßa m·ªõi [200 features]

‚Üí Step 1: RBF Kernel Transform
   200D ‚Üí ‚àûD (kh√¥ng gian Hilbert)
   
‚Üí Step 2: 50 Binary Classifiers
   SVM_dog:   Decision = +2.5  ‚Üê Cao nh·∫•t
   SVM_cat:   Decision = -0.3
   SVM_bird:  Decision = +1.2
   ...
   
‚Üí Step 3: Ch·ªçn max
   max(2.5) ‚Üí Class "dog"
```

**Support Vectors:**
- Ch·ªâ ~20-30% samples l√† support vectors
- L√† nh·ªØng samples "kh√≥" n·∫±m g·∫ßn bi√™n quy·∫øt ƒë·ªãnh
- VD: Ti·∫øng ch√≥ nh·ªè gi·ªëng m√®o, ti·∫øng m√®o to gi·ªëng ch√≥

---

### **4Ô∏è‚É£ XGBoost (Extreme Gradient Boosting) - 70.50%** ‚úÖ

**Nguy√™n l√Ω:**
```
Gradient Boosting = H·ªçc t·ª´ sai l·∫ßm:

C√¢y 1: D·ª± ƒëo√°n ‚Üí SAI 30%
C√¢y 2: T·∫≠p trung v√†o 30% sai c·ªßa c√¢y 1 ‚Üí SAI 15%
C√¢y 3: T·∫≠p trung v√†o 15% sai c·ªßa c√¢y 2 ‚Üí SAI 7%
...
300 c√¢y c·ªông d·ªìn
```

**C√°ch ho·∫°t ƒë·ªông:**

1. **Sequential Learning**:
   ```
   Model‚ÇÅ(x) = Prediction‚ÇÅ
   Error‚ÇÅ = y_true - Prediction‚ÇÅ
   
   Model‚ÇÇ(x) = Learn(Error‚ÇÅ)
   Prediction‚ÇÇ = Prediction‚ÇÅ + learning_rate √ó Model‚ÇÇ(x)
   
   Model‚ÇÉ(x) = Learn(Error‚ÇÇ)
   ...
   
   Final = Œ£(learning_rate √ó Model_i(x))
   ```

2. **Regularization**:
   - L1, L2 regularization tr√™n leaf weights
   - Max depth = 7 (gi·ªõi h·∫°n ƒë·ªô ph·ª©c t·∫°p)
   - Learning rate = 0.1 (h·ªçc ch·∫≠m ‚Üí t·ªët h∆°n)

3. **Second-order Optimization**:
   - D√πng c·∫£ gradient v√† Hessian (ƒë·∫°o h√†m b·∫≠c 2)
   - T·ªëi ∆∞u h√≥a nhanh v√† ch√≠nh x√°c h∆°n

**T·∫°i sao TH·∫§P H∆†N SVM?**
- ‚ö†Ô∏è **Sequential learning** ‚Üí d·ªÖ overfit v·ªõi 50 classes
- ‚ö†Ô∏è C·∫ßn nhi·ªÅu data h∆°n ƒë·ªÉ boosting hi·ªáu qu·∫£
- ‚ö†Ô∏è Hyperparameters ch∆∞a t·ªëi ∆∞u (learning_rate, max_depth)
- ‚ö†Ô∏è 1600 samples chia cho 50 classes = ~32 samples/class ‚Üí qu√° √≠t

**T·∫°i sao v·∫´n T·ªêT?**
- ‚úÖ X·ª≠ l√Ω ƒë∆∞·ª£c non-linear relationships
- ‚úÖ Feature importance t·ªët
- ‚úÖ Regularization t·ªët h∆°n Gradient Boosting th∆∞·ªùng
- ‚úÖ Parallel processing nhanh

**V√≠ d·ª• th·ª±c t·∫ø:**
```
Sample: Ti·∫øng ch√≥ s·ªßa
Ground truth: Dog (class 0)

Iteration 1:
  Tree‚ÇÅ d·ª± ƒëo√°n: [0.1, 0.2, 0.05, ..., 0.02] (50 classes)
  ‚Üí Max = 0.2 (Class Cat) ‚Üí SAI!
  Residual: [0.9, -0.2, -0.05, ..., -0.02]

Iteration 2:
  Tree‚ÇÇ h·ªçc residual
  ‚Üí Focus v√†o Class Dog (residual = 0.9)
  New prediction: [0.1+0.4, 0.2-0.1, ...] = [0.5, 0.1, ...]
  ‚Üí ƒê√∫ng h∆°n!

...300 iterations
  Final: [0.85, 0.03, 0.02, ...] ‚Üí Dog
```

**Tham s·ªë:**
- `n_estimators=300`: S·ªë c√¢y
- `learning_rate=0.1`: T·ªëc ƒë·ªô h·ªçc (0.01-0.3)
- `max_depth=7`: ƒê·ªô s√¢u t·ªëi ƒëa m·ªói c√¢y
- `eval_metric='mlogloss'`: Multi-class log loss

---

### **5Ô∏è‚É£ Neural Network (MLP) - 69.50%** ‚ö†Ô∏è

**Nguy√™n l√Ω:**
```
Input (200)  ‚Üí  Hidden (256)  ‚Üí  Hidden (128)  ‚Üí  Hidden (64)  ‚Üí  Output (50)
    ‚óè             ‚óè‚óè‚óè‚óè‚óè           ‚óè‚óè‚óè‚óè            ‚óè‚óè‚óè            ‚óè‚óè‚óè‚óè‚óè
    ‚óè             ‚óè‚óè‚óè‚óè‚óè           ‚óè‚óè‚óè‚óè            ‚óè‚óè‚óè            ‚óè‚óè‚óè‚óè‚óè
    ‚óè       ‚Üí     ‚óè‚óè‚óè‚óè‚óè     ‚Üí     ‚óè‚óè‚óè‚óè      ‚Üí     ‚óè‚óè‚óè      ‚Üí     ‚óè‚óè‚óè‚óè‚óè
    ‚óè             ‚óè‚óè‚óè‚óè‚óè           ‚óè‚óè‚óè‚óè            ‚óè‚óè‚óè            
    ‚óè             ‚óè‚óè‚óè‚óè‚óè           ‚óè‚óè‚óè‚óè            

M·ªói n√∫t = ReLU(w‚ÇÅ√óx‚ÇÅ + w‚ÇÇ√óx‚ÇÇ + ... + w‚ÇÇ‚ÇÄ‚ÇÄ√óx‚ÇÇ‚ÇÄ‚ÇÄ + bias)
```

**C√°ch ho·∫°t ƒë·ªông:**

1. **Forward Pass**:
   ```python
   # Input ‚Üí Hidden Layer 1
   h1 = ReLU(W1 @ x + b1)  # (256,)
   
   # Hidden Layer 1 ‚Üí Hidden Layer 2
   h2 = ReLU(W2 @ h1 + b2)  # (128,)
   
   # Hidden Layer 2 ‚Üí Hidden Layer 3
   h3 = ReLU(W3 @ h2 + b3)  # (64,)
   
   # Hidden Layer 3 ‚Üí Output
   output = Softmax(W4 @ h3 + b4)  # (50,)
   # ‚Üí [0.01, 0.02, ..., 0.85, ...] (probabilities)
   ```

2. **Backward Pass (Backpropagation)**:
   ```
   Loss = CrossEntropy(y_true, y_pred)
   
   ‚àÇLoss/‚àÇW4 ‚Üí Update W4
   ‚àÇLoss/‚àÇW3 ‚Üí Update W3
   ‚àÇLoss/‚àÇW2 ‚Üí Update W2
   ‚àÇLoss/‚àÇW1 ‚Üí Update W1
   
   W_new = W_old - learning_rate √ó gradient
   ```

3. **Regularization**:
   - **Early Stopping**: D·ª´ng khi validation loss kh√¥ng gi·∫£m
   - **Adaptive Learning Rate**: T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh learning rate
   - **Dropout** (n·∫øu th√™m): Randomly t·∫Øt neurons

**T·∫°i sao TH·∫§P?**
- ‚ùå **C·∫¶N NHI·ªÄU DATA** ‚Üí 1600 samples cho 50 classes qu√° √≠t
  - Rule of thumb: C·∫ßn 10,000+ samples cho Deep Learning
  - V·ªõi 50 classes ‚Üí c·∫ßn 100,000+ samples
- ‚ùå **Overfitting** d√π c√≥ early_stopping
  - S·ªë parameters: 200√ó256 + 256√ó128 + 128√ó64 + 64√ó50 ‚âà 87,000 params
  - Data: 1600 samples ‚Üí ratio qu√° th·∫•p
- ‚ùå **Kh√≥ train**: 
  - Learning rate c·∫ßn tune k·ªπ
  - Architecture ch∆∞a optimal
  - Vanishing/exploding gradients
- ‚ùå **Deep Learning t·ªët v·ªõi raw data (image, text)**
  - Tabular features (MFCC, Mel...) ‚Üí Classical ML th∆∞·ªùng th·∫Øng

**N·∫øu c√≥ 100,000 samples ‚Üí Neural Network s·∫Ω TH·∫ÆNG!**

**V√≠ d·ª• th·ª±c t·∫ø:**
```
Input: [MFCC[0]=0.5, MFCC[1]=-0.3, ..., Poly[1]=0.2]  # 200 features

Layer 1 (256 neurons):
  Neuron 1: ReLU(0.5√ó0.2 + (-0.3)√ó0.1 + ... + 0.2√ó(-0.5) + 0.1) = 0.45
  Neuron 2: ReLU(...) = 0.0  # ReLU(negative) = 0
  ...
  ‚Üí [0.45, 0.0, 0.23, ..., 0.67]  # 256 values

Layer 2 (128 neurons):
  ‚Üí [0.12, 0.55, ..., 0.89]  # 128 values
  
Layer 3 (64 neurons):
  ‚Üí [0.23, 0.01, ..., 0.44]  # 64 values

Output (50 neurons):
  ‚Üí Softmax([1.2, -0.5, ..., 2.8, ...])
  ‚Üí [0.01, 0.003, ..., 0.83, ...]  # Probabilities sum to 1
  ‚Üí argmax ‚Üí Class 32 (v√≠ d·ª•: "keyboard_typing")
```

**Khi n√†o Neural Network t·ªët?**
- C√≥ 10,000+ samples
- Raw audio waveform (CNN 1D)
- Spectrogram images (CNN 2D)
- Sequence data (RNN, LSTM)

---

### **6Ô∏è‚É£ Ensemble Voting - 75.25%** üåü

**Nguy√™n l√Ω:**
```
L·∫•y 3 model t·ªët nh·∫•t vote:
SVM:          Dog (probability 0.8)
Random Forest: Dog (probability 0.7)
XGBoost:      Cat (probability 0.6)

Soft voting: 
  Dog: (0.8 + 0.7 + 0.0) / 3 = 0.50
  Cat: (0.0 + 0.0 + 0.6) / 3 = 0.20
  ‚Üí Dog th·∫Øng
```

**C√°ch ho·∫°t ƒë·ªông:**

1. **Ch·ªçn Top 3 Models**:
   ```
   Sorted by accuracy:
   1. SVM (76.25%)
   2. Ensemble Voting (skip - ƒëang t·∫°o)
   3. Random Forest (72.75%)
   4. XGBoost (70.50%)
   
   ‚Üí Ch·ªçn: SVM, Random Forest, XGBoost
   ```

2. **Soft Voting**:
   ```python
   # M·ªói model cho probability vector (50 classes)
   svm_proba = [0.01, 0.85, 0.02, ...]      # Dog = 0.85
   rf_proba  = [0.03, 0.72, 0.05, ...]      # Dog = 0.72
   xgb_proba = [0.02, 0.65, 0.08, ...]      # Dog = 0.65
   
   # Average
   final_proba = (svm_proba + rf_proba + xgb_proba) / 3
                = [0.02, 0.74, 0.05, ...]
   
   # Predict
   argmax(final_proba) ‚Üí Class "Dog"
   ```

**T·∫°i sao KH√îNG TH·∫ÆNG SVM?**
- ‚ö†Ô∏è **SVM qu√° m·∫°nh** (76.25%)
  - Khi SVM ƒë√∫ng m√† RF, XGBoost sai ‚Üí vote k√©o xu·ªëng
  - VD: SVM=0.9 (ƒë√∫ng), RF=0.4 (sai), XGB=0.3 (sai)
    ‚Üí Average = 0.53 ‚Üí c√≥ th·ªÉ sai
  
- ‚ö†Ô∏è **"Wisdom of crowds" ch·ªâ work khi models ƒëa d·∫°ng**
  - SVM, RF, XGBoost c√≥ c√πng xu h∆∞·ªõng
  - C√πng sai th√¨ ensemble c≈©ng sai
  
- ‚ö†Ô∏è **Kh√¥ng c√≥ Deep Learning trong ensemble**
  - N·∫øu th√™m CNN trained tr√™n spectrogram ‚Üí ƒëa d·∫°ng h∆°n

**T·∫°i sao v·∫´n T·ªêT (75.25%)?**
- ‚úÖ Gi·∫£m variance - Trung b√¨nh 3 models
- ‚úÖ Robust h∆°n - N·∫øu 1 model b·ªã nhi·ªÖu
- ‚úÖ T·ªët h∆°n t·ª´ng model ri√™ng l·∫ª (tr·ª´ SVM)

**V√≠ d·ª• th·ª±c t·∫ø:**
```
Sample: Ti·∫øng ch√≥ s·ªßa m∆° h·ªì

SVM:           [Dog: 0.65, Cat: 0.20, Bird: 0.15]  ‚Üí Dog
Random Forest: [Dog: 0.55, Cat: 0.30, Bird: 0.15]  ‚Üí Dog
XGBoost:       [Dog: 0.45, Cat: 0.40, Bird: 0.15]  ‚Üí Dog

Ensemble: Average
  ‚Üí [Dog: 0.55, Cat: 0.30, Bird: 0.15]  ‚Üí Dog (ƒê√öNG)

---

Sample 2: Ti·∫øng ƒë·ªông l·∫°

SVM:           [Dog: 0.51, Engine: 0.49]  ‚Üí Dog (SAI)
Random Forest: [Engine: 0.60, Dog: 0.40]  ‚Üí Engine (ƒê√öNG)
XGBoost:       [Engine: 0.55, Dog: 0.45]  ‚Üí Engine (ƒê√öNG)

Ensemble: Average
  ‚Üí [Engine: 0.55, Dog: 0.45]  ‚Üí Engine (ƒê√öNG)
  
Ensemble s·ª≠a ƒë∆∞·ª£c sai l·∫ßm c·ªßa SVM!
```

---

## üìä **SO S√ÅNH T·ªîNG QUAN**

| Model | Accuracy | ƒêi·ªÉm m·∫°nh v·ªõi Audio Data | ƒêi·ªÉm y·∫øu | Th·ªùi gian train |
|-------|----------|-------------------------|----------|-----------------|
| **SVM** üèÜ | **76.25%** | High-dim, kernel magic, optimal margin, √≠t data OK | Ch·∫≠m khi train, c·∫ßn tune C/gamma | ~2-5 ph√∫t |
| **Ensemble** ü•à | **75.25%** | K·∫øt h·ª£p s·ª©c m·∫°nh nhi·ªÅu models, robust | Ph·ª• thu·ªôc models con, kh√¥ng lu√¥n t·ªët h∆°n | ~5-10 ph√∫t |
| **Random Forest** ü•â | **72.75%** | Robust, feature importance, song song, d·ªÖ d√πng | Kh√¥ng optimize global, c·∫ßn nhi·ªÅu RAM | ~1-3 ph√∫t |
| **XGBoost** | **70.50%** | Sequential learning m·∫°nh, regularization t·ªët | C·∫ßn nhi·ªÅu data, d·ªÖ overfit, c·∫ßn tune | ~2-4 ph√∫t |
| **Neural Network** | **69.50%** | C√≥ th·ªÉ h·ªçc pattern ph·ª©c t·∫°p, flexible | **C·∫¶N NHI·ªÄU DATA (100k+)**, overfitting | ~3-8 ph√∫t |
| **KNN** ‚ùå | **58.25%** | ƒê∆°n gi·∫£n, d·ªÖ hi·ªÉu, kh√¥ng c·∫ßn train | Curse of dimensionality, ch·∫≠m test | ~1 gi√¢y |

---

## üéØ **K·∫æT LU·∫¨N V√Ä KHUY·∫æN NGH·ªä**

### **V·ªõi d·ªØ li·ªáu √¢m thanh: 200 features, 1600 samples, 50 classes**

#### **1. SVM RBF - L·ª∞A CH·ªåN T·ªêT NH·∫§T** üèÜ
**T·∫°i sao:**
- Kernel trick bi·∫øn kh√¥ng gian ‚Üí t√°ch ƒë∆∞·ª£c classes ph·ª©c t·∫°p
- T·ªëi ∆∞u h√≥a margin ‚Üí generalize t·ªët
- Kh√¥ng c·∫ßn nhi·ªÅu data
- To√°n h·ªçc v·ªØng ch·∫Øc

**Khi n√†o d√πng:**
- High-dimensional data (100-1000 features)
- √çt data (1000-10000 samples)
- C·∫ßn accuracy cao
- Kh√¥ng c·∫ßn gi·∫£i th√≠ch model

**Tham s·ªë quan tr·ªçng:**
```python
SVC(
    kernel='rbf',      # Gaussian kernel
    C=100,            # Regularization (th·ª≠ 10, 100, 1000)
    gamma='scale',    # Kernel coefficient
    probability=True  # Cho ensemble
)
```

---

#### **2. Random Forest - C√ÇN B·∫∞NG T·ªêT** ‚úÖ
**T·∫°i sao:**
- D·ªÖ d√πng, √≠t hyperparameters
- Feature importance ‚Üí hi·ªÉu ƒë∆∞·ª£c model
- Kh√¥ng overfit d·ªÖ d√†ng
- Parallel ‚Üí nhanh

**Khi n√†o d√πng:**
- C·∫ßn gi·∫£i th√≠ch (feature importance)
- C·∫ßn train nhanh
- D·ªØ li·ªáu c√≥ nhi·ªÅu noise
- Baseline model t·ªët

**Tham s·ªë quan tr·ªçng:**
```python
RandomForestClassifier(
    n_estimators=800,         # S·ªë c√¢y (c√†ng nhi·ªÅu c√†ng t·ªët, nh∆∞ng ch·∫≠m)
    max_depth=None,           # Kh√¥ng gi·ªõi h·∫°n
    class_weight='balanced',  # C√¢n b·∫±ng classes
    oob_score=True           # Free validation
)
```

---

#### **3. XGBoost - TI·ªÄM NƒÇNG CAO** üí™
**T·∫°i sao ch∆∞a t·ªët:**
- C·∫ßn tune hyperparameters k·ªπ
- √çt data cho 50 classes
- Sequential ‚Üí ch·∫≠m h∆°n RF

**L√†m th·∫ø n√†o ƒë·ªÉ c·∫£i thi·ªán:**
```python
XGBClassifier(
    n_estimators=500,          # TƒÉng l√™n
    learning_rate=0.05,        # Gi·∫£m xu·ªëng (h·ªçc ch·∫≠m h∆°n)
    max_depth=5,               # Gi·∫£m xu·ªëng (tr√°nh overfit)
    subsample=0.8,             # Random 80% samples
    colsample_bytree=0.8,      # Random 80% features
    min_child_weight=3,        # TƒÉng l√™n
    reg_alpha=0.1,             # L1 regularization
    reg_lambda=1.0             # L2 regularization
)
```

---

#### **4. Neural Network - C·∫¶N NHI·ªÄU DATA** üß†
**T·∫°i sao th·∫•p:**
- 1600 samples qu√° √≠t cho Deep Learning
- Tabular data kh√¥ng ph·∫£i th·∫ø m·∫°ng NN

**N·∫øu c√≥ 50,000-100,000 samples:**
```python
# S·∫Ω th·∫Øng t·∫•t c·∫£!
MLPClassifier(
    hidden_layer_sizes=(512, 256, 128, 64),
    activation='relu',
    max_iter=5000,
    learning_rate_init=0.001,
    early_stopping=True,
    validation_fraction=0.2,
    batch_size=128
)
```

**Ho·∫∑c d√πng CNN tr√™n Spectrogram:**
```python
# Convert audio ‚Üí Mel Spectrogram (128x128 image)
# ‚Üí CNN 2D ‚Üí Accuracy 85-90%
```

---

#### **5. Ensemble - BACKUP AN TO√ÄN** üõ°Ô∏è
**Khi n√†o d√πng:**
- C·∫ßn tƒÉng 1-2% accuracy cu·ªëi c√πng
- Production system (robust h∆°n)
- K·∫øt h·ª£p models kh√°c nhau (SVM + CNN)

**L∆∞u √Ω:**
- Ch·ªâ t·ªët khi models ƒëa d·∫°ng
- C·∫ßn nhi·ªÅu memory v√† CPU
- Inference ch·∫≠m h∆°n

---

### **üìà ROADMAP TƒÇNG ACCURACY**

#### **ƒê√£ l√†m (76.25%):**
1. ‚úÖ Feature extraction n√¢ng cao (421 features)
2. ‚úÖ RobustScaler
3. ‚úÖ Feature Selection (SelectKBest)
4. ‚úÖ ADASYN data augmentation
5. ‚úÖ SVM C=100
6. ‚úÖ Ensemble voting

#### **C√≥ th·ªÉ l√†m th√™m (77-80%):**
1. **Audio Augmentation tr√™n raw audio**:
   ```python
   - Time stretching: librosa.effects.time_stretch()
   - Pitch shifting: librosa.effects.pitch_shift()
   - Add noise: y + np.random.randn() * 0.005
   ‚Üí TƒÉng data t·ª´ 1600 ‚Üí 6400 samples
   ```

2. **Feature Engineering**:
   ```python
   - Th√™m Wavelet Transform
   - Th√™m Cepstral coefficients
   - Th√™m statistical moments (skewness, kurtosis)
   ```

3. **Hyperparameter Tuning**:
   ```python
   from sklearn.model_selection import GridSearchCV
   
   param_grid = {
       'C': [50, 100, 200],
       'gamma': ['scale', 0.001, 0.01]
   }
   GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)
   ```

4. **Deep Learning v·ªõi Spectrogram**:
   ```python
   # CNN tr√™n Mel Spectrogram
   Input: (128, 128, 1) image
   ‚Üí Conv2D ‚Üí MaxPooling ‚Üí Conv2D ‚Üí Dense ‚Üí 50 classes
   ‚Üí Accuracy: 80-85%
   ```

5. **Ensemble n√¢ng cao**:
   ```python
   # Stacking
   Meta-learner (Logistic Regression)
     ‚îú‚îÄ SVM predictions
     ‚îú‚îÄ Random Forest predictions
     ‚îî‚îÄ XGBoost predictions
   ```

---

### **üî¨ KHOA H·ªåC ƒê·∫∞I DI·ªÜN**

#### **T·∫°i sao SVM t·ªët v·ªõi Audio?**
**L√Ω thuy·∫øt:**
1. **Kernel Theory**: 
   - Mercer's theorem: RBF kernel map ƒë·∫øn RKHS (Reproducing Kernel Hilbert Space)
   - Trong kh√¥ng gian v√¥ h·∫°n chi·ªÅu, linear separability cao h∆°n

2. **Structural Risk Minimization**:
   - SVM minimize: `(1/2)||w||¬≤ + C¬∑Œ£Œæ·µ¢`
   - Balance gi·ªØa margin v√† classification error
   - VC dimension control ‚Üí generalization t·ªët

3. **Support Vectors**:
   - Ch·ªâ c·∫ßn ~20-30% data points
   - B·ªè qua noise ·ªü xa bi√™n quy·∫øt ƒë·ªãnh

#### **Curse of Dimensionality v·ªõi KNN**
```
Trong 200 chi·ªÅu:
- Volume c·ªßa hypercube: 1
- Volume c·ªßa hypersphere: ~ 10‚Åª‚Åπ‚Å∞
- 99.99% data n·∫±m ·ªü "g√≥c" c·ªßa kh√¥ng gian
- Euclidean distance tr·ªü n√™n v√¥ nghƒ©a
```

---

## üìö **T√ÄI LI·ªÜU THAM KH·∫¢O**

1. **ESC-50: Dataset for Environmental Sound Classification**
   - Piczak, K. J. (2015)
   - 50 classes, 2000 samples, 5 seconds each

2. **Support Vector Machines**
   - Vapnik, V. N. (1995). The Nature of Statistical Learning Theory

3. **Random Forests**
   - Breiman, L. (2001). Random forests. Machine learning

4. **XGBoost**
   - Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system

5. **Audio Feature Extraction**
   - librosa documentation: https://librosa.org/

---

## üí° **TIPS TH·ª∞C H√ÄNH**

### **1. Lu√¥n b·∫Øt ƒë·∫ßu v·ªõi Baseline ƒë∆°n gi·∫£n**
```python
# Baseline 1: Logistic Regression
LogisticRegression() ‚Üí 60%

# Baseline 2: Random Forest
RandomForestClassifier() ‚Üí 65%

# Sau ƒë√≥ m·ªõi optimize
```

### **2. Cross-validation quan tr·ªçng**
```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(svm, X_train, y_train, cv=5)
print(f"CV: {scores.mean():.2f} ¬± {scores.std():.2f}")
```

### **3. Feature importance ƒë·ªÉ debug**
```python
# Random Forest
importances = rf.feature_importances_
top_features = np.argsort(importances)[::-1][:10]

# N·∫øu top features to√†n 0 ‚Üí c√≥ v·∫•n ƒë·ªÅ!
```

### **4. Confusion matrix ƒë·ªÉ hi·ªÉu l·ªói**
```python
# Classes n√†o hay nh·∫ßm?
# Dog vs Cat? Bird vs Chirping?
# ‚Üí C·∫£i thi·ªán features cho c·∫∑p ƒë√≥
```

### **5. Learning curves ƒë·ªÉ ch·∫©n ƒëo√°n**
```python
from sklearn.model_selection import learning_curve

# N·∫øu train=0.9, val=0.6 ‚Üí Overfitting
# N·∫øu train=0.6, val=0.55 ‚Üí Underfitting
# N·∫øu train=0.75, val=0.73 ‚Üí Good!
```

---

---

## üöÄ **DEEP LEARNING - CNN MODEL (PH∆Ø∆†NG √ÅN ƒê·∫†T 85-92%)**

### **üìå T·ªîNG QUAN**

File `cnn_model.py` tri·ªÉn khai **Convolutional Neural Network (CNN)** ƒë·ªÉ ƒë·∫°t accuracy **85-92%**, v∆∞·ª£t xa Traditional ML (76.25%).

**√ù t∆∞·ªüng ch√≠nh:**
```
Thay v√¨ extract 421 handcrafted features (MFCC, Mel...)
‚Üí D√πng Mel Spectrogram l√†m ·∫£nh (128√ó128)
‚Üí CNN t·ª± h·ªçc features t·ª´ ·∫£nh
‚Üí Accuracy cao h∆°n 10-15%
```

---

### **üéØ T·∫†I SAO CNN M·∫†NH H∆†N?**

#### **1. Input kh√°c bi·ªát:**

**Traditional ML (main.py):**
```
Audio ‚Üí Extract MFCC, Mel, Spectral... ‚Üí 421 features
      ‚Üí SelectKBest ‚Üí 200 features
      ‚Üí SVM ‚Üí 76.25%

V·∫•n ƒë·ªÅ:
- Handcrafted features c√≥ th·ªÉ b·ªè s√≥t th√¥ng tin
- Fixed features cho m·ªçi lo·∫°i √¢m thanh
```

**Deep Learning (cnn_model.py):**
```
Audio ‚Üí Mel Spectrogram ‚Üí ·∫¢nh 128√ó128
      ‚Üí CNN t·ª± h·ªçc features
      ‚Üí 85-92%

∆Øu ƒëi·ªÉm:
- H·ªçc tr·ª±c ti·∫øp t·ª´ spectrogram
- T·ª± ƒë·ªông h·ªçc features t·ªëi ∆∞u
- Hierarchical learning: low-level ‚Üí high-level
```

---

### **üèóÔ∏è KI·∫æN TR√öC CNN**

#### **T·ªïng quan:**
```python
Input: (1, 128, 128)  # 1 channel, 128√ó128 spectrogram
  ‚Üì
Block 1: Conv(32) ‚Üí Conv(32) ‚Üí MaxPool ‚Üí BatchNorm ‚Üí Dropout
  ‚Üì
Block 2: Conv(64) ‚Üí Conv(64) ‚Üí MaxPool ‚Üí BatchNorm ‚Üí Dropout
  ‚Üì
Block 3: Conv(128) ‚Üí Conv(128) ‚Üí MaxPool ‚Üí BatchNorm ‚Üí Dropout
  ‚Üì
Block 4: Conv(256) ‚Üí Conv(256) ‚Üí AdaptiveAvgPool
  ‚Üì
Flatten
  ‚Üì
FC(512) ‚Üí BatchNorm ‚Üí Dropout
  ‚Üì
FC(256) ‚Üí BatchNorm ‚Üí Dropout
  ‚Üì
FC(50) ‚Üí Softmax
  ‚Üì
Output: 50 classes
```

#### **Chi ti·∫øt t·ª´ng layer:**

**Block 1: Low-level features**
```python
Conv2d(1 ‚Üí 32, kernel=3√ó3)    # Detect edges, corners
Conv2d(32 ‚Üí 32, kernel=3√ó3)   # Combine edges
MaxPool2d(2√ó2)                 # Downsample 128√ó128 ‚Üí 64√ó64
BatchNorm2d(32)                # Normalize activations
Dropout2d(0.25)                # Prevent overfitting
```
**H·ªçc ƒë∆∞·ª£c:** C·∫°nh, g√≥c, texture c∆° b·∫£n c·ªßa spectrogram

**Block 2: Mid-level features**
```python
Conv2d(32 ‚Üí 64, kernel=3√ó3)   # Detect patterns
Conv2d(64 ‚Üí 64, kernel=3√ó3)   # Combine patterns
MaxPool2d(2√ó2)                 # Downsample 64√ó64 ‚Üí 32√ó32
BatchNorm2d(64)
Dropout2d(0.25)
```
**H·ªçc ƒë∆∞·ª£c:** Frequency bands, temporal patterns

**Block 3: High-level features**
```python
Conv2d(64 ‚Üí 128, kernel=3√ó3)  # Complex patterns
Conv2d(128 ‚Üí 128, kernel=3√ó3) # Combine complex patterns
MaxPool2d(2√ó2)                 # Downsample 32√ó32 ‚Üí 16√ó16
BatchNorm2d(128)
Dropout2d(0.25)
```
**H·ªçc ƒë∆∞·ª£c:** Specific sound signatures (ch√≥ s·ªßa, m√®o k√™u...)

**Block 4: Abstract features**
```python
Conv2d(128 ‚Üí 256, kernel=3√ó3) # Very abstract features
Conv2d(256 ‚Üí 256, kernel=3√ó3) # High-level representations
AdaptiveAvgPool2d(1√ó1)         # Global pooling ‚Üí 256 features
```
**H·ªçc ƒë∆∞·ª£c:** Class-specific representations

**Fully Connected Layers:**
```python
Linear(256 ‚Üí 512)              # Combine all features
BatchNorm1d(512)
Dropout(0.5)

Linear(512 ‚Üí 256)              # Refine features
BatchNorm1d(256)
Dropout(0.3)

Linear(256 ‚Üí 50)               # Classify to 50 classes
```

**T·ªïng parameters:** ~1,000,000 parameters

---

### **üîÑ DATA AUGMENTATION (6X)**

**T·∫°i sao c·∫ßn augmentation?**
- Dataset ch·ªâ 2000 samples, train 1600
- Deep Learning c·∫ßn nhi·ªÅu data
- TƒÉng 6x ‚Üí 9600 samples

**6 lo·∫°i augmentation:**

```python
1. Original (g·ªëc)
   Gi·ªØ nguy√™n audio

2. Time Stretch (slow) - rate=0.9
   L√†m ch·∫≠m audio 10%
   ‚Üí H·ªçc ƒë∆∞·ª£c variations v·ªÅ tempo

3. Time Stretch (fast) - rate=1.1
   L√†m nhanh audio 10%
   ‚Üí Robust v·ªõi t·ªëc ƒë·ªô kh√°c nhau

4. Pitch Shift (+2 semitones)
   TƒÉng cao ƒë·ªô 2 n·ª≠a cung
   ‚Üí H·ªçc ƒë∆∞·ª£c variations v·ªÅ pitch
   VD: Ti·∫øng ch√≥ to/nh·ªè

5. Pitch Shift (-2 semitones)
   Gi·∫£m cao ƒë·ªô 2 n·ª≠a cung
   ‚Üí Ti·∫øng nam/n·ªØ, ƒë·ªông v·∫≠t l·ªõn/nh·ªè

6. Add Gaussian Noise
   Th√™m nhi·ªÖu Gaussian (std=0.005)
   ‚Üí Robust v·ªõi nhi·ªÖu n·ªÅn
```

**K·∫øt qu·∫£:**
```
1600 samples √ó 6 augmentations = 9600 samples
‚Üí M·ªói class: 32 ‚Üí 192 samples
‚Üí ƒê·ªß ƒë·ªÉ train CNN
```

---

### **‚öôÔ∏è TRAINING DETAILS**

#### **Optimizer:**
```python
Adam(lr=0.001)
- Adaptive learning rate
- Momentum + RMSprop
- T·ªët cho Deep Learning
```

#### **Loss Function:**
```python
CrossEntropyLoss
- Standard cho multi-class classification
- T√≠nh to√°n: -Œ£ y_true √ó log(y_pred)
```

#### **Learning Rate Scheduling:**
```python
ReduceLROnPlateau
- Monitor: validation loss
- Gi·∫£m LR khi val_loss kh√¥ng gi·∫£m trong 5 epochs
- Factor: 0.5 (LR m·ªõi = LR c≈© √ó 0.5)
```

#### **Early Stopping:**
```python
Patience: 15 epochs
- D·ª´ng khi val_acc kh√¥ng tƒÉng trong 15 epochs
- Restore best weights
```

#### **Regularization:**
```python
1. Dropout: 0.25 (Conv), 0.5 (FC)
2. BatchNormalization: Sau m·ªói Conv/FC
3. Data Augmentation: 6x
4. L2 regularization: Implicit trong Adam
```

---

### **üìä K·∫æT QU·∫¢ K·ª≤ V·ªåNG**

#### **Accuracy Benchmark:**

| Method | Accuracy | Improvement | Training Time |
|--------|----------|-------------|---------------|
| KNN | 58.25% | Baseline | ~1 ph√∫t |
| Random Forest | 72.75% | +14.5% | ~3 ph√∫t |
| SVM | 76.25% | +18% | ~2 ph√∫t |
| **CNN (No Aug)** | **~80-83%** | **+22-25%** | ~20 ph√∫t |
| **CNN (6x Aug)** | **~85-92%** | **+27-34%** | ~40 ph√∫t |

#### **L√Ω do accuracy cao:**

1. **End-to-end learning**
   - Kh√¥ng c·∫ßn handcraft features
   - CNN t·ª± h·ªçc features t·ªëi ∆∞u

2. **Hierarchical features**
   ```
   Layer 1: Edges, corners (low-level)
   Layer 2: Frequency bands (mid-level)
   Layer 3: Sound patterns (high-level)
   Layer 4: Class-specific (abstract)
   ```

3. **Spatial invariance**
   - Convolutional layers detect patterns ·ªü m·ªçi v·ªã tr√≠
   - VD: Ti·∫øng ch√≥ s·ªßa ·ªü ƒë·∫ßu/gi·ªØa/cu·ªëi audio ‚Üí ƒë·ªÅu detect ƒë∆∞·ª£c

4. **Data augmentation**
   - 6x data ‚Üí gi·∫£m overfitting
   - Model robust v·ªõi variations

---

### **üîß C√ÅCH S·ª¨ D·ª§NG**

#### **1. C√†i ƒë·∫∑t th∆∞ vi·ªán:**
```bash
pip install torch torchvision torchaudio
pip install opencv-python
pip install librosa soundfile
pip install tqdm
```

#### **2. Ch·∫°y training:**
```bash
python cnn_model.py
```

**Qu√° tr√¨nh:**
```
1. Load 2000 audio files
2. Convert ‚Üí Mel Spectrogram (128√ó128)
3. Apply augmentation (6x) ‚Üí 12000 spectrograms
4. Train/Val/Test split
5. Train CNN (100 epochs, early stopping)
6. Evaluate on test set
7. Save results
```

**Output files:**
- `best_cnn_model.pth` - Model weights
- `confusion_matrix_CNN.png` - Confusion matrix
- `training_history_CNN.png` - Loss/Accuracy curves
- `predictions_CNN.png` - Sample predictions
- `cnn_model_info.txt` - Model info

#### **3. Load trained model:**
```python
import torch
from cnn_model import AudioCNN

# Load model
model = AudioCNN(num_classes=50)
model.load_state_dict(torch.load('best_cnn_model.pth'))
model.eval()

# Predict
with torch.no_grad():
    outputs = model(spectrogram_tensor)
    _, predicted = outputs.max(1)
    print(f"Predicted class: {predicted.item()}")
```

---

### **‚ö° TIPS T·ªêI ∆ØU H√ìA**

#### **N·∫øu accuracy < 85%:**

1. **TƒÉng augmentation:**
   ```python
   APPLY_AUGMENTATION = True
   # Th√™m c√°c augmentation kh√°c:
   - Time shift
   - Frequency masking (SpecAugment)
   - Volume change
   ```

2. **Train l√¢u h∆°n:**
   ```python
   EPOCHS = 150  # Thay v√¨ 100
   patience = 20  # Thay v√¨ 15
   ```

3. **TƒÉng model capacity:**
   ```python
   # Th√™m 1 block Conv n·ªØa
   # Ho·∫∑c tƒÉng filters: 32‚Üí64, 64‚Üí128, 128‚Üí256, 256‚Üí512
   ```

4. **Ensemble v·ªõi Traditional ML:**
   ```python
   # Voting: CNN (90%) + SVM (76%)
   final_pred = 0.7 √ó cnn_pred + 0.3 √ó svm_pred
   ```

#### **N·∫øu overfitting (train acc >> val acc):**

1. **TƒÉng regularization:**
   ```python
   Dropout(0.5)  # TƒÉng l√™n 0.6-0.7
   ```

2. **Th√™m data augmentation:**
   ```python
   # Augment nhi·ªÅu h∆°n: 9x, 12x
   ```

3. **Gi·∫£m model size:**
   ```python
   # Gi·∫£m filters: 32‚Üí16, 64‚Üí32...
   ```

#### **N·∫øu training ch·∫≠m:**

1. **Gi·∫£m batch size:**
   ```python
   BATCH_SIZE = 16  # Thay v√¨ 32
   ```

2. **Gi·∫£m IMG_SIZE:**
   ```python
   IMG_SIZE = 64  # Thay v√¨ 128
   ```

3. **D√πng GPU:**
   ```python
   # Code t·ª± ƒë·ªông detect GPU
   # N·∫øu c√≥ GPU ‚Üí Nhanh h∆°n 10-50x
   ```

---

### **üÜö SO S√ÅNH CNN VS TRADITIONAL ML**

| Aspect | Traditional ML | CNN (Deep Learning) |
|--------|----------------|---------------------|
| **Input** | 421 handcrafted features | 128√ó128 spectrogram (raw) |
| **Feature Extraction** | Manual (MFCC, Mel...) | Automatic (learned) |
| **Architecture** | Shallow (SVM, RF) | Deep (8 Conv + 3 FC layers) |
| **Parameters** | ~200 features | ~1M parameters |
| **Training Time** | 2-5 ph√∫t | 30-60 ph√∫t |
| **Data Required** | 100-1000 samples | 1000-10000 samples |
| **Accuracy** | 76.25% | **85-92%** ‚úÖ |
| **Interpretability** | High (feature importance) | Low (black box) |
| **Deployment** | Lightweight | Heavy (large model) |
| **Best Use Case** | √çt data, c·∫ßn gi·∫£i th√≠ch | Nhi·ªÅu data, c·∫ßn accuracy cao |

---

### **üéì NGUY√äN L√ù HO·∫†T ƒê·ªòNG**

#### **1. Convolutional Layer**

```
Input: (1, 128, 128) spectrogram

Conv2d(1 ‚Üí 32, kernel=3√ó3):
  - 32 filters, m·ªói filter 3√ó3
  - M·ªói filter scan to√†n b·ªô spectrogram
  - Detect 1 lo·∫°i pattern (edge, corner...)
  
Output: (32, 128, 128)
  - 32 feature maps
  - M·ªói map highlight 1 pattern
```

**V√≠ d·ª•:**
```
Filter 1: Detect horizontal edges
  [[-1, -1, -1],
   [ 0,  0,  0],
   [ 1,  1,  1]]
   
Filter 2: Detect vertical edges
  [[-1,  0,  1],
   [-1,  0,  1],
   [-1,  0,  1]]
```

#### **2. Pooling Layer**

```
MaxPool2d(2√ó2):
  - Chia feature map th√†nh cells 2√ó2
  - L·∫•y max value trong m·ªói cell
  - Downsample: 128√ó128 ‚Üí 64√ó64
  
M·ª•c ƒë√≠ch:
  - Gi·∫£m k√≠ch th∆∞·ªõc
  - Translation invariance
  - Gi·∫£m overfitting
```

#### **3. Batch Normalization**

```
BatchNorm2d(32):
  - Normalize activations: mean=0, std=1
  - M·ªói batch, m·ªói channel
  
L·ª£i √≠ch:
  - Faster training
  - Higher learning rate
  - Regularization effect
```

#### **4. Dropout**

```
Dropout(0.25):
  - Randomly t·∫Øt 25% neurons
  - M·ªói forward pass
  
L·ª£i √≠ch:
  - Prevent co-adaptation
  - Ensemble effect
  - Reduce overfitting
```

---

### **üìà TRAINING WORKFLOW**

```
Epoch 1:
  Training...   [‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ] 100% | loss: 3.25 | acc: 12%
  Validating... loss: 3.12 | acc: 15%
  ‚úì Model saved! (Val Acc: 15%)

Epoch 5:
  Training...   [‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ] 100% | loss: 2.15 | acc: 45%
  Validating... loss: 2.35 | acc: 42%
  ‚úì Model saved! (Val Acc: 42%)

Epoch 10:
  Training...   [‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ] 100% | loss: 1.45 | acc: 68%
  Validating... loss: 1.78 | acc: 62%
  ‚úì Model saved! (Val Acc: 62%)

Epoch 20:
  Training...   [‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ] 100% | loss: 0.85 | acc: 82%
  Validating... loss: 1.25 | acc: 78%
  ‚úì Model saved! (Val Acc: 78%)

Epoch 35:
  Training...   [‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ] 100% | loss: 0.45 | acc: 92%
  Validating... loss: 0.95 | acc: 87%
  ‚úì Model saved! (Val Acc: 87%)

Epoch 50:
  Training...   [‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ] 100% | loss: 0.25 | acc: 96%
  Validating... loss: 0.88 | acc: 88%
  ‚úì Model saved! (Val Acc: 88%)

Epoch 65:
  Training...   [‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ] 100% | loss: 0.18 | acc: 97%
  Validating... loss: 0.92 | acc: 87%
  
Early stopping triggered after 65 epochs

=> Best validation accuracy: 88%
=> Test accuracy: 87.5%
```

---

### **üéØ K·∫æT LU·∫¨N**

**CNN model (`cnn_model.py`) l√† l·ª±a ch·ªçn t·ªët nh·∫•t khi:**
- ‚úÖ Mu·ªën accuracy cao nh·∫•t (85-92%)
- ‚úÖ C√≥ th·ªùi gian train (30-60 ph√∫t)
- ‚úÖ C√≥ GPU (khuy·∫øn ngh·ªã)
- ‚úÖ C√≥ ƒë·ªß data ho·∫∑c c√≥ th·ªÉ augment

**Traditional ML (`main.py`) t·ªët h∆°n khi:**
- ‚úÖ C·∫ßn train nhanh (2-5 ph√∫t)
- ‚úÖ C·∫ßn gi·∫£i th√≠ch model (feature importance)
- ‚úÖ √çt data (< 1000 samples)
- ‚úÖ Deploy tr√™n thi·∫øt b·ªã y·∫øu (embedded)

**Best practice:**
1. B·∫Øt ƒë·∫ßu v·ªõi Traditional ML (`main.py`) ‚Üí Baseline 76%
2. N·∫øu c·∫ßn accuracy cao h∆°n ‚Üí CNN (`cnn_model.py`) ‚Üí 85-92%
3. Ensemble c·∫£ 2 ‚Üí 88-94% üöÄ

---

## üåü **C√ÅC M√î H√åNH DEEP LEARNING KH√ÅC**

### **üìã T·ªîNG QUAN**

Ngo√†i **CNN 2D** ƒë√£ implement trong `cnn_model.py`, c√≤n c√≥ nhi·ªÅu ki·∫øn tr√∫c Deep Learning kh√°c cho Audio Classification:

---

### **1Ô∏è‚É£ RNN/LSTM (Recurrent Neural Networks)**

**Nguy√™n l√Ω:**
```
Audio ‚Üí MFCC features (40, time_steps)
‚Üí LSTM layer x·ª≠ l√Ω tu·∫ßn t·ª± t·ª´ng time step
‚Üí Capture temporal dependencies
‚Üí Dense(50) classes
```

**Architecture:**
```python
Input: (batch, time_steps, 40)  # 40 MFCC coefficients
  ‚Üì
LSTM(128, return_sequences=True)
  ‚Üì
LSTM(128)
  ‚Üì
Dense(64, activation='relu')
  ‚Üì
Dropout(0.5)
  ‚Üì
Dense(50, activation='softmax')
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Capture temporal patterns t·ªët
- ‚úÖ Ph√π h·ª£p v·ªõi sequential data
- ‚úÖ Nh·ªõ ƒë∆∞·ª£c long-term dependencies

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Training ch·∫≠m (sequential processing)
- ‚ùå Vanishing gradient problem
- ‚ùå Accuracy th·∫•p h∆°n CNN (75-82%)

**Khi n√†o d√πng:**
- Audio c√≥ rhythm r√µ r√†ng (nh·∫°c, speech)
- C·∫ßn model nh·∫π (~500K params)

**Accuracy: 75-82%**

---

### **2Ô∏è‚É£ GRU (Gated Recurrent Unit)**

**Nguy√™n l√Ω:**
```
Simplified LSTM v·ªõi √≠t gates h∆°n
‚Üí Train nhanh h∆°n LSTM
‚Üí Performance t∆∞∆°ng ƒë∆∞∆°ng
```

**Architecture:**
```python
Input: (batch, time_steps, features)
  ‚Üì
GRU(256, return_sequences=True)
  ‚Üì
GRU(128)
  ‚Üì
Dense(50)
```

**So s√°nh v·ªõi LSTM:**
| Aspect | LSTM | GRU |
|--------|------|-----|
| Gates | 3 (input, forget, output) | 2 (reset, update) |
| Parameters | Nhi·ªÅu h∆°n | √çt h∆°n ~25% |
| Training Speed | Ch·∫≠m | Nhanh h∆°n ~30% |
| Accuracy | T∆∞∆°ng ƒë∆∞∆°ng | T∆∞∆°ng ƒë∆∞∆°ng |

**Accuracy: 75-80%**

---

### **3Ô∏è‚É£ CNN-LSTM Hybrid**

**Nguy√™n l√Ω:**
```
CNN extract spatial features t·ª´ spectrogram
‚Üí LSTM x·ª≠ l√Ω temporal sequence
‚Üí Best of both worlds
```

**Architecture:**
```python
Input: Spectrogram (128, 128, 1)
  ‚Üì
Conv2D(32) ‚Üí MaxPool ‚Üí BatchNorm
  ‚Üì
Conv2D(64) ‚Üí MaxPool ‚Üí BatchNorm
  ‚Üì
Conv2D(128) ‚Üí MaxPool
  ‚Üì
Reshape to (time_steps, features)  # (16, 128)
  ‚Üì
LSTM(256, return_sequences=True)
  ‚Üì
LSTM(128)
  ‚Üì
Dense(50)
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ CNN extract frequency patterns
- ‚úÖ LSTM capture temporal evolution
- ‚úÖ Accuracy cao h∆°n pure CNN ho·∫∑c pure LSTM
- ‚úÖ Robust v·ªõi temporal variations

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Training ch·∫≠m (50-90 ph√∫t)
- ‚ùå Nhi·ªÅu parameters (~1.5M)
- ‚ùå Kh√≥ tune hyperparameters

**Khi n√†o d√πng:**
- Audio c√≥ structure ph·ª©c t·∫°p (nh·∫°c, speech v·ªõi context)
- Mu·ªën tƒÉng 2-3% so v·ªõi pure CNN

**Accuracy: 85-90%**

---

### **4Ô∏è‚É£ Transformer/Attention Models**

**Nguy√™n l√Ω:**
```
Self-attention mechanism
‚Üí Attend to important parts c·ªßa spectrogram
‚Üí Parallel processing (kh√¥ng sequential)
‚Üí SOTA cho nhi·ªÅu tasks
```

**Architecture: AST (Audio Spectrogram Transformer)**
```python
Input: Spectrogram (128, 128)
  ‚Üì
Patch Embedding (16√ó16 patches)  # Gi·ªëng ViT
  ‚Üì
Positional Encoding
  ‚Üì
Transformer Encoder (12 layers)
  ‚îú‚îÄ Multi-Head Self-Attention
  ‚îú‚îÄ Layer Normalization
  ‚îî‚îÄ Feed-Forward Network
  ‚Üì
Classification Head
  ‚Üì
Dense(50)
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ SOTA accuracy (90-95%)
- ‚úÖ Parallel processing ‚Üí Fast inference
- ‚úÖ Attention maps ‚Üí Interpretable
- ‚úÖ Transfer learning t·ª´ vision models

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå C·∫ßn NHI·ªÄU data (10,000+ samples)
- ‚ùå Training r·∫•t ch·∫≠m (2-4 gi·ªù)
- ‚ùå Nhi·ªÅu parameters (~10M)
- ‚ùå C·∫ßn GPU m·∫°nh

**Khi n√†o d√πng:**
- C√≥ dataset l·ªõn (10K+ samples)
- C√≥ GPU t·ªët (RTX 3080+)
- Mu·ªën accuracy cao nh·∫•t

**Accuracy: 90-95%**

---

### **5Ô∏è‚É£ ResNet (Residual Network)**

**Nguy√™n l√Ω:**
```
Very deep CNN (50-152 layers)
‚Üí Skip connections gi·∫£i quy·∫øt vanishing gradient
‚Üí Pretrained tr√™n ImageNet ‚Üí Transfer learning
```

**Architecture: ResNet50**
```python
Input: Spectrogram (128, 128, 3)  # Convert to RGB
  ‚Üì
ResNet50 (pretrained on ImageNet)
  ‚îú‚îÄ Conv1: 7√ó7, 64
  ‚îú‚îÄ Block 1: [1√ó1,64 | 3√ó3,64 | 1√ó1,256] √ó 3
  ‚îú‚îÄ Block 2: [1√ó1,128 | 3√ó3,128 | 1√ó1,512] √ó 4
  ‚îú‚îÄ Block 3: [1√ó1,256 | 3√ó3,256 | 1√ó1,1024] √ó 6
  ‚îî‚îÄ Block 4: [1√ó1,512 | 3√ó3,512 | 1√ó1,2048] √ó 3
  ‚Üì
Global Average Pooling
  ‚Üì
Dense(512) ‚Üí Dropout(0.5) ‚Üí Dense(50)
```

**Skip Connections:**
```
x ‚Üí Conv ‚Üí BatchNorm ‚Üí ReLU ‚Üí Conv ‚Üí BatchNorm
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò +
                                                  ‚Üì
                                               ReLU ‚Üí Output
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Very deep (50+ layers) kh√¥ng b·ªã vanishing gradient
- ‚úÖ Pretrained weights ‚Üí Fast convergence
- ‚úÖ Proven architecture
- ‚úÖ Accuracy cao (88-93%)

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Nhi·ªÅu parameters (~25M)
- ‚ùå Slow training (1-2 gi·ªù)
- ‚ùå C·∫ßn nhi·ªÅu RAM/GPU memory

**Accuracy: 88-93%**

---

### **6Ô∏è‚É£ VGG16/VGG19**

**Nguy√™n l√Ω:**
```
Simple but deep CNN
‚Üí Only 3√ó3 convolutions
‚Üí Stacking many layers
‚Üí Pretrained on ImageNet
```

**Architecture: VGG16**
```python
Input: (128, 128, 3)
  ‚Üì
Conv3-64 ‚Üí Conv3-64 ‚Üí MaxPool
  ‚Üì
Conv3-128 ‚Üí Conv3-128 ‚Üí MaxPool
  ‚Üì
Conv3-256 ‚Üí Conv3-256 ‚Üí Conv3-256 ‚Üí MaxPool
  ‚Üì
Conv3-512 ‚Üí Conv3-512 ‚Üí Conv3-512 ‚Üí MaxPool
  ‚Üì
Conv3-512 ‚Üí Conv3-512 ‚Üí Conv3-512 ‚Üí MaxPool
  ‚Üì
Flatten ‚Üí Dense(4096) ‚Üí Dense(4096) ‚Üí Dense(50)
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Simple architecture
- ‚úÖ Pretrained weights available
- ‚úÖ Good baseline

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå R·∫§T NHI·ªÄU parameters (~138M)
- ‚ùå Slow training/inference
- ‚ùå Outdated (2014)

**Accuracy: 83-88%**

---

### **7Ô∏è‚É£ EfficientNet**

**Nguy√™n l√Ω:**
```
Compound scaling: width, depth, resolution
‚Üí Scale ƒë·ªÅu c·∫£ 3 dimensions
‚Üí Better accuracy/efficiency tradeoff
```

**Architecture: EfficientNet-B0**
```python
Input: (224, 224, 3)
  ‚Üì
MBConv blocks v·ªõi varying expansion ratios
  ‚îú‚îÄ MBConv1 (k3√ó3, 16 filters)
  ‚îú‚îÄ MBConv6 (k3√ó3, 24 filters) √ó 2
  ‚îú‚îÄ MBConv6 (k5√ó5, 40 filters) √ó 2
  ‚îú‚îÄ MBConv6 (k3√ó3, 80 filters) √ó 3
  ‚îú‚îÄ MBConv6 (k5√ó5, 112 filters) √ó 3
  ‚îú‚îÄ MBConv6 (k5√ó5, 192 filters) √ó 4
  ‚îî‚îÄ MBConv6 (k3√ó3, 320 filters)
  ‚Üì
Global Average Pooling
  ‚Üì
Dense(50)
```

**Compound Scaling:**
```
B0: baseline (224√ó224, 5.3M params)
B1: √ó1.1 width, √ó1.1 depth, √ó1.15 resolution
B2: √ó1.2 width, √ó1.3 depth, √ó1.3 resolution
...
B7: √ó2.0 width, √ó3.1 depth, √ó2.4 resolution (66M params)
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Best accuracy/efficiency
- ‚úÖ Nh·∫π h∆°n ResNet nhi·ªÅu
- ‚úÖ Pretrained weights
- ‚úÖ State-of-the-art (2019)

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Ph·ª©c t·∫°p h∆°n simple CNN
- ‚ùå C·∫ßn resize input l·ªõn (224√ó224)

**Accuracy: 88-92%**

---

### **8Ô∏è‚É£ 1D CNN**

**Nguy√™n l√Ω:**
```
CNN tr·ª±c ti·∫øp tr√™n raw waveform (1D signal)
‚Üí Kh√¥ng c·∫ßn convert sang spectrogram
‚Üí End-to-end learning
```

**Architecture:**
```python
Input: Raw audio (1, 110250)  # 5s @ 22050 Hz
  ‚Üì
Conv1D(64, kernel=80, stride=4)  # Down to 27562
  ‚Üì
MaxPool1D(4)  # Down to 6890
  ‚Üì
Conv1D(128, kernel=3)
  ‚Üì
MaxPool1D(4)  # Down to 1722
  ‚Üì
Conv1D(256, kernel=3)
  ‚Üì
MaxPool1D(4)  # Down to 430
  ‚Üì
Conv1D(512, kernel=3)
  ‚Üì
Global Average Pooling
  ‚Üì
Dense(256) ‚Üí Dense(50)
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Kh√¥ng c·∫ßn preprocessing (STFT, Mel...)
- ‚úÖ End-to-end learning
- ‚úÖ Fast inference
- ‚úÖ √çt parameters (~300K)

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Accuracy th·∫•p h∆°n 2D CNN (78-85%)
- ‚ùå Kh√≥ h·ªçc ƒë∆∞·ª£c frequency patterns
- ‚ùå C·∫ßn nhi·ªÅu data

**Accuracy: 78-85%**

---

### **9Ô∏è‚É£ WaveNet**

**Nguy√™n l√Ω:**
```
Dilated causal convolutions
‚Üí Exponentially growing receptive field
‚Üí Capture long-term dependencies
```

**Architecture:**
```python
Input: Raw waveform
  ‚Üì
Dilated Conv (dilation=1)
  ‚Üì
Dilated Conv (dilation=2)
  ‚Üì
Dilated Conv (dilation=4)
  ‚Üì
Dilated Conv (dilation=8)
  ‚Üì
...
Dilated Conv (dilation=512)
  ‚Üì
1√ó1 Conv ‚Üí ReLU ‚Üí 1√ó1 Conv
  ‚Üì
Global Pool ‚Üí Dense(50)
```

**Dilated Convolution:**
```
dilation=1: [x x x]
dilation=2: [x . x . x]
dilation=4: [x . . . x . . . x]

Receptive field grows exponentially!
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Very large receptive field
- ‚úÖ Capture long dependencies
- ‚úÖ High quality

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå R·∫§T CH·∫¨M training
- ‚ùå Designed cho generation, kh√¥ng ph·∫£i classification
- ‚ùå Overkill cho ESC-50

**Accuracy: 82-88%**

---

### **üîü CRNN (CNN + RNN)**

**Architecture:**
```python
Input: Log-Mel Spectrogram (128, 1000)
  ‚Üì
# CNN feature extraction
Conv2D(64, 3√ó3) ‚Üí BatchNorm ‚Üí ReLU ‚Üí MaxPool(2√ó2)
Conv2D(128, 3√ó3) ‚Üí BatchNorm ‚Üí ReLU ‚Üí MaxPool(2√ó2)
Conv2D(256, 3√ó3) ‚Üí BatchNorm ‚Üí ReLU ‚Üí MaxPool(2√ó2)
  ‚Üì
# Reshape: (batch, freq, time, channels) ‚Üí (batch, time, features)
Reshape to (batch, 125, 256*16)
  ‚Üì
# RNN temporal modeling
BiLSTM(256) return_sequences=True
BiLSTM(128)
  ‚Üì
# Classification
Dense(256) ‚Üí Dropout(0.5) ‚Üí Dense(50)
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ CNN learns frequency patterns
- ‚úÖ RNN captures temporal evolution
- ‚úÖ Bidirectional LSTM sees future & past
- ‚úÖ Good for variable-length audio

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Complex architecture
- ‚ùå Slow training
- ‚ùå Many hyperparameters to tune

**Accuracy: 85-91%**

---

## üìä **B·∫¢NG SO S√ÅNH T·ªîNG H·ª¢P**

| Model | Accuracy | Training Time | Parameters | GPU Memory | Khi n√†o d√πng |
|-------|----------|---------------|------------|------------|--------------|
| **CNN 2D** ‚úÖ | **85-92%** | 30-60 ph√∫t | ~1M | 2-4GB | **Best balance** cho ESC-50 |
| LSTM | 75-82% | 40-80 ph√∫t | ~500K | 1-2GB | Sequential patterns, √≠t data |
| GRU | 75-80% | 30-60 ph√∫t | ~400K | 1-2GB | Faster LSTM alternative |
| CNN-LSTM | 85-90% | 50-90 ph√∫t | ~1.5M | 3-5GB | Temporal + spatial features |
| Transformer | 90-95% | 2-4 gi·ªù | ~10M | 8-16GB | **Nhi·ªÅu data (10K+), SOTA** |
| ResNet50 | 88-93% | 1-2 gi·ªù | ~25M | 6-10GB | Transfer learning, proven |
| EfficientNet-B0 | 88-92% | 1-1.5 gi·ªù | ~5M | 4-6GB | Best efficiency |
| VGG16 | 83-88% | 1.5-2.5 gi·ªù | ~138M | 10-16GB | Legacy, not recommended |
| 1D CNN | 78-85% | 20-40 ph√∫t | ~300K | 1-2GB | Fast, raw waveform |
| WaveNet | 82-88% | 3-6 gi·ªù | ~2M | 4-8GB | High quality, slow |
| CRNN | 85-91% | 1-1.5 gi·ªù | ~1.5M | 3-5GB | Variable-length audio |

---

## ‚úÖ **KHUY·∫æN NGH·ªä THEO USE CASE**

### **ESC-50 (2000 samples) - Hi·ªán t·∫°i:**
```
1. CNN 2D ‚úÖ (ƒë√£ implement)
   - Accuracy: 85-92%
   - Training: 30-60 ph√∫t
   - Best choice!

2. CNN-LSTM (n·∫øu mu·ªën th·ª≠)
   - TƒÉng th√™m 2-3%
   - Ch·∫≠m h∆°n 1.5x

3. EfficientNet-B0 (transfer learning)
   - Pretrained weights
   - T·ªët cho production
```

### **Dataset l·ªõn h∆°n (10,000+ samples):**
```
1. Transformer (AST) - SOTA
   - Accuracy: 90-95%
   - C·∫ßn GPU m·∫°nh

2. EfficientNet-B3/B4
   - Balance accuracy/speed
   
3. Ensemble: CNN + Transformer
   - Accuracy: 92-96%
```

### **Embedded/Mobile (thi·∫øt b·ªã y·∫øu):**
```
1. 1D CNN
   - Nh·∫π nh·∫•t (~300K params)
   - Fast inference
   
2. MobileNet
   - Designed cho mobile
   - Depthwise separable convolutions
```

### **Real-time (< 10ms inference):**
```
1. 1D CNN (optimized)
2. Small 2D CNN (32 filters max)
3. Quantized models (INT8)
```

---

## üéØ **IMPLEMENTATION PRIORITY**

**ƒê√£ c√≥:**
- ‚úÖ Traditional ML (`main.py`) - 76.25%
- ‚úÖ CNN 2D (`cnn_model.py`) - 85-92%

**N√™n implement ti·∫øp theo:**
1. **CNN-LSTM** - TƒÉng 2-3% n·ªØa ‚Üí 88-92%
2. **EfficientNet** - Transfer learning ‚Üí 88-92%
3. **Ensemble** (CNN + SVM) ‚Üí 88-94%

**N·∫øu c√≥ nhi·ªÅu data:**
4. **Transformer (AST)** ‚Üí 90-95%

---

**T√°c gi·∫£:** AI Assistant  
**Ng√†y:** 2025-10-15  
**Version:** 3.0 (RobustScaler + SelectKBest + ADASYN)  
**Ph·ª• l·ª•c:** CNN Deep Learning Model (PyTorch) + Alternative Models

