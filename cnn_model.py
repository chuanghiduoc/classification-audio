# -*- coding: utf-8 -*-
"""
PH√ÇN LO·∫†I √ÇM THANH B·∫∞NG CNN - DEEP LEARNING
S·ª≠ d·ª•ng Mel Spectrogram l√†m input cho CNN 2D v·ªõi PyTorch
M·ª•c ti√™u: 85-92% accuracy
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import librosa
import librosa.display
import cv2
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
from tqdm import tqdm

# PyTorch
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.backends.cudnn as cudnn

# =============================================================================
# C·∫§U H√åNH
# =============================================================================

DATA_PATH = 'data/audio/audio/'
CSV_PATH = 'data/esc50.csv'
IMG_SIZE = 128
APPLY_AUGMENTATION = True  # Data augmentation (time/pitch shift, noise)
APPLY_SPECAUGMENT = True  # SpecAugment (frequency/time masking) 
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print("="*70)
print("DEEP LEARNING - CNN MODEL (PyTorch)")
print("="*70)
print(f"PyTorch version: {torch.__version__}")
print(f"Device: {DEVICE}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
else:
    print("GPU: Not available (using CPU)")

# =============================================================================
# SPECAUGMENT - FREQUENCY & TIME MASKING
# =============================================================================

class SpecAugment:
    """
    SpecAugment: Ph∆∞∆°ng ph√°p tƒÉng c∆∞·ªùng d·ªØ li·ªáu ƒë∆°n gi·∫£n cho ASR (Google Research)
    https://arxiv.org/abs/1904.08779
    
    √Åp d·ª•ng masking ng·∫´u nhi√™n l√™n c√°c chi·ªÅu t·∫ßn s·ªë v√† th·ªùi gian c·ªßa mel spectrogram.
    Gi√∫p gi·∫£m overfitting v√† c·∫£i thi·ªán kh·∫£ nƒÉng t·ªïng qu√°t h√≥a c·ªßa m√¥ h√¨nh.
    """
    def __init__(self, freq_mask_param=20, time_mask_param=20, n_freq_masks=2, n_time_masks=2):
        """
        Kh·ªüi t·∫°o SpecAugment
        
        Args:
            freq_mask_param: ƒê·ªô r·ªông t·ªëi ƒëa c·ªßa frequency mask (s·ªë bins t·∫ßn s·ªë)
            time_mask_param: ƒê·ªô r·ªông t·ªëi ƒëa c·ªßa time mask (s·ªë frames th·ªùi gian)
            n_freq_masks: S·ªë l∆∞·ª£ng frequency masks s·∫Ω √°p d·ª•ng
            n_time_masks: S·ªë l∆∞·ª£ng time masks s·∫Ω √°p d·ª•ng
        """
        self.freq_mask_param = freq_mask_param
        self.time_mask_param = time_mask_param
        self.n_freq_masks = n_freq_masks
        self.n_time_masks = n_time_masks
    
    def __call__(self, spec):
        """
        √Åp d·ª•ng SpecAugment l√™n mel spectrogram
        
        Args:
            spec: M·∫£ng mel spectrogram v·ªõi ƒë·ªãnh d·∫°ng (H, W) ho·∫∑c (C, H, W)
                  H = chi·ªÅu cao (frequency bins)
                  W = chi·ªÅu r·ªông (time frames)
                  C = channels (th∆∞·ªùng l√† 1)
            
        Returns:
            Mel spectrogram ƒë√£ ƒë∆∞·ª£c augment (c√≥ c√°c v√πng b·ªã mask = 0)
        """
        spec = spec.copy()  # T·∫°o b·∫£n sao ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng d·ªØ li·ªáu g·ªëc
        
        # X·ª≠ l√Ω c·∫£ ƒë·ªãnh d·∫°ng (H, W) v√† (C, H, W)
        if len(spec.shape) == 3:
            spec = spec.squeeze(0)  # B·ªè chi·ªÅu channel n·∫øu c√≥
            had_channel = True
        else:
            had_channel = False
        
        num_freq_bins, num_time_frames = spec.shape
        
        # Frequency masking: Che ng·∫´u nhi√™n c√°c d·∫£i t·∫ßn s·ªë
        # Gi√∫p model h·ªçc ƒë∆∞·ª£c c√°c ƒë·∫∑c tr∆∞ng t·∫ßn s·ªë ƒëa d·∫°ng h∆°n
        for _ in range(self.n_freq_masks):
            f = np.random.randint(0, self.freq_mask_param)  # ƒê·ªô r·ªông mask ng·∫´u nhi√™n
            f0 = np.random.randint(0, num_freq_bins - f)    # V·ªã tr√≠ b·∫Øt ƒë·∫ßu mask
            spec[f0:f0 + f, :] = 0  # Set c√°c gi√° tr·ªã trong v√πng mask = 0
        
        # Time masking: Che ng·∫´u nhi√™n c√°c khung th·ªùi gian
        # Gi√∫p model h·ªçc ƒë∆∞·ª£c c√°c ƒë·∫∑c tr∆∞ng th·ªùi gian ƒëa d·∫°ng h∆°n
        for _ in range(self.n_time_masks):
            t = np.random.randint(0, self.time_mask_param)  # ƒê·ªô r·ªông mask ng·∫´u nhi√™n
            t0 = np.random.randint(0, num_time_frames - t)  # V·ªã tr√≠ b·∫Øt ƒë·∫ßu mask
            spec[:, t0:t0 + t] = 0  # Set c√°c gi√° tr·ªã trong v√πng mask = 0
        
        # Kh√¥i ph·ª•c chi·ªÅu channel n·∫øu ban ƒë·∫ßu c√≥
        if had_channel:
            spec = np.expand_dims(spec, axis=0)
        
        return spec

# =============================================================================
# H√ÄM TR√çCH XU·∫§T MEL SPECTROGRAM
# =============================================================================

def extract_mel_spectrogram(file_path, img_size=128, augment=False):
    """
    Tr√≠ch xu·∫•t Mel Spectrogram t·ª´ file audio v√† chuy·ªÉn th√†nh ·∫£nh 2D
    
    Quy tr√¨nh: Audio (1D) ‚Üí STFT ‚Üí Mel filterbank ‚Üí Log scale ‚Üí Normalize ‚Üí Resize ‚Üí ·∫¢nh 2D
    
    Args:
        file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file audio (.wav)
        img_size: K√≠ch th∆∞·ªõc ·∫£nh output (m·∫∑c ƒë·ªãnh 128x128)
        augment: N·∫øu True, s·∫Ω t·∫°o th√™m 5 phi√™n b·∫£n augmented (t·ªïng 6 ·∫£nh t·ª´ 1 audio)
    
    Returns:
        - N·∫øu augment=False: Tr·∫£ v·ªÅ 1 m·∫£ng numpy (128, 128)
        - N·∫øu augment=True: Tr·∫£ v·ªÅ list g·ªìm 6 m·∫£ng numpy (m·ªói m·∫£ng 128x128)
            1. Original (g·ªëc)
            2. Time stretch slow (ch·∫≠m 0.9x)
            3. Time stretch fast (nhanh 1.1x)
            4. Pitch shift up (+2 b√°n cung)
            5. Pitch shift down (-2 b√°n cung)
            6. Gaussian noise (th√™m nhi·ªÖu Gaussian)
    """
    try:
        # B∆∞·ªõc 1: Load audio v·ªõi sample rate 22050 Hz, c·∫Øt v·ªÅ 5 gi√¢y
        # sr = sample rate (s·ªë m·∫´u m·ªói gi√¢y)
        # y = m·∫£ng numpy ch·ª©a d·ªØ li·ªáu √¢m thanh
        y, sr = librosa.load(file_path, sr=22050, duration=5.0)
        
        # B∆∞·ªõc 2: N·∫øu audio ng·∫Øn h∆°n 5s ‚Üí th√™m zeros v√†o cu·ªëi ƒë·ªÉ ƒë·ªß 5s
        # ƒê·∫£m b·∫£o t·∫•t c·∫£ audio c√≥ c√πng ƒë·ªô d√†i cho CNN
        if len(y) < sr * 5:
            y = np.pad(y, (0, sr * 5 - len(y)), mode='constant')
        
        results = []  # Danh s√°ch l∆∞u c√°c mel spectrogram
        
        # B∆∞·ªõc 3: T·∫°o mel spectrogram cho audio g·ªëc
        mel_spec = create_mel_spectrogram(y, sr, img_size)
        results.append(mel_spec)
        
        # B∆∞·ªõc 4: Data Augmentation - T·∫°o th√™m 5 phi√™n b·∫£n bi·∫øn th·ªÉ
        # Gi√∫p tƒÉng s·ªë l∆∞·ª£ng d·ªØ li·ªáu train, gi·∫£m overfitting
        if augment:
            # 1. Time Stretching (ch·∫≠m l·∫°i 0.9x): Gi·ªëng ng∆∞·ªùi n√≥i ch·∫≠m h∆°n
            y_slow = librosa.effects.time_stretch(y, rate=0.9)
            mel_spec_slow = create_mel_spectrogram(y_slow, sr, img_size)
            results.append(mel_spec_slow)
            
            # 2. Time Stretching (nhanh l√™n 1.1x): Gi·ªëng ng∆∞·ªùi n√≥i nhanh h∆°n
            y_fast = librosa.effects.time_stretch(y, rate=1.1)
            mel_spec_fast = create_mel_spectrogram(y_fast, sr, img_size)
            results.append(mel_spec_fast)
            
            # 3. Pitch Shifting (+2 semitones): TƒÉng cao ƒë·ªô l√™n (gi·ªçng cao h∆°n)
            y_pitch_up = librosa.effects.pitch_shift(y, sr=sr, n_steps=2)
            mel_spec_pitch_up = create_mel_spectrogram(y_pitch_up, sr, img_size)
            results.append(mel_spec_pitch_up)
            
            # 4. Pitch Shifting (-2 semitones): Gi·∫£m cao ƒë·ªô xu·ªëng (gi·ªçng th·∫•p h∆°n)
            y_pitch_down = librosa.effects.pitch_shift(y, sr=sr, n_steps=-2)
            mel_spec_pitch_down = create_mel_spectrogram(y_pitch_down, sr, img_size)
            results.append(mel_spec_pitch_down)
            
            # 5. Th√™m Gaussian Noise: M√¥ ph·ªèng nhi·ªÖu m√¥i tr∆∞·ªùng (ti·∫øng ·ªìn n·ªÅn)
            noise = np.random.randn(len(y)) * 0.005  # Nhi·ªÖu nh·ªè (œÉ=0.005)
            y_noise = y + noise
            mel_spec_noise = create_mel_spectrogram(y_noise, sr, img_size)
            results.append(mel_spec_noise)
        
        # Tr·∫£ v·ªÅ list n·∫øu augment, ho·∫∑c m·∫£ng ƒë∆°n n·∫øu kh√¥ng augment
        return results if augment else results[0]
        
    except Exception as e:
        print(f"Loi: {file_path}: {e}")
        return None

def create_mel_spectrogram(y, sr, img_size):
    """
    T·∫°o Mel Spectrogram t·ª´ audio signal v√† chuy·ªÉn th√†nh ·∫£nh chu·∫©n h√≥a
    
    Args:
        y: Audio time series (m·∫£ng 1D)
        sr: Sample rate (Hz)
        img_size: K√≠ch th∆∞·ªõc ·∫£nh output (128x128)
    
    Returns:
        Mel spectrogram ƒë√£ chu·∫©n h√≥a v·ªÅ [0,1] v·ªõi k√≠ch th∆∞·ªõc (img_size, img_size)
    """
    # B∆∞·ªõc 1: T√≠nh Mel Spectrogram
    # n_mels=128: S·ªë l∆∞·ª£ng mel bands (chi·ªÅu cao c·ªßa spectrogram)
    # n_fft=2048: K√≠ch th∆∞·ªõc FFT window (ƒë·ªô ph√¢n gi·∫£i t·∫ßn s·ªë)
    # hop_length=512: S·ªë m·∫´u gi·ªØa c√°c frame li√™n ti·∫øp (ƒë·ªô ph√¢n gi·∫£i th·ªùi gian)
    # fmax=8000: T·∫ßn s·ªë t·ªëi ƒëa c·∫ßn ph√¢n t√≠ch (8kHz ƒë·ªß cho √¢m thanh m√¥i tr∆∞·ªùng)
    mel_spec = librosa.feature.melspectrogram(
        y=y,
        sr=sr,
        n_mels=128,      # 128 mel frequency bands
        n_fft=2048,      # Window size cho FFT
        hop_length=512,  # B∆∞·ªõc nh·∫£y gi·ªØa c√°c window
        fmax=8000        # T·∫ßn s·ªë t·ªëi ƒëa
    )
    
    # B∆∞·ªõc 2: Chuy·ªÉn sang thang ƒëo dB (decibel) - thang logarithmic
    # Gi·ªëng c√°ch tai ng∆∞·ªùi nghe √¢m thanh (phi tuy·∫øn t√≠nh)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    
    # B∆∞·ªõc 3: Chu·∫©n h√≥a v·ªÅ kho·∫£ng [0, 1]
    # Gi√∫p CNN h·ªçc t·ªët h∆°n (input trong kho·∫£ng chu·∫©n)
    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min())
    
    # B∆∞·ªõc 4: Resize v·ªÅ k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh (128x128)
    # ƒê·∫£m b·∫£o t·∫•t c·∫£ ·∫£nh c√≥ c√πng k√≠ch th∆∞·ªõc cho CNN
    mel_spec_resized = cv2.resize(mel_spec_norm, (img_size, img_size))
    
    return mel_spec_resized

# =============================================================================
# ƒê·ªåC D·ªÆ LI·ªÜU - CHIA TRAIN/VAL/TEST TR∆Ø·ªöC KHI AUGMENT
# =============================================================================

print("\n" + "="*70)
print("TRICH XUAT MEL SPECTROGRAMS")
print("="*70)

df = pd.read_csv(CSV_PATH)
print(f"Dataset: {len(df)} samples, {df['target'].nunique()} classes")

# B∆Ø·ªöC 1: Chia Train/Test tr∆∞·ªõc (80/20)
train_val_df, test_df = train_test_split(
    df, test_size=0.2, random_state=42, stratify=df['target']
)

# B∆Ø·ªöC 2: Chia Train/Val t·ª´ ph·∫ßn train_val (80/20 c·ªßa 80% = 64% train, 16% val)
train_df, val_df = train_test_split(
    train_val_df, test_size=0.2, random_state=42, stratify=train_val_df['target']
)

print(f"\n=> Split:")
print(f"   Train: {len(train_df)} files ({len(train_df)/len(df)*100:.1f}%)")
print(f"   Val:   {len(val_df)} files ({len(val_df)/len(df)*100:.1f}%)")
print(f"   Test:  {len(test_df)} files ({len(test_df)/len(df)*100:.1f}%)")

# B∆Ø·ªöC 3: X·ª≠ l√Ω TRAINING set (c√≥ augmentation)
print(f"\n[1/3] Xu ly TRAIN set ({len(train_df)} files)...")
if APPLY_AUGMENTATION:
    print("      Augmentation: BAT (6x)")

train_spectrograms = []
train_labels = []

for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc="Train"):
    file_path = os.path.join(DATA_PATH, row['filename'])
    label = row['target']
    
    result = extract_mel_spectrogram(file_path, IMG_SIZE, augment=APPLY_AUGMENTATION)
    
    if result is not None:
        if APPLY_AUGMENTATION:
            for spec in result:
                train_spectrograms.append(spec)
                train_labels.append(label)
        else:
            train_spectrograms.append(result)
            train_labels.append(label)

# B∆Ø·ªöC 4: X·ª≠ l√Ω VALIDATION set (KH√îNG augmentation)
print(f"\n[2/3] Xu ly VAL set ({len(val_df)} files)...")
print("      Augmentation: TAT")

val_spectrograms = []
val_labels = []

for idx, row in tqdm(val_df.iterrows(), total=len(val_df), desc="Val"):
    file_path = os.path.join(DATA_PATH, row['filename'])
    label = row['target']
    
    result = extract_mel_spectrogram(file_path, IMG_SIZE, augment=False)
    
    if result is not None:
        val_spectrograms.append(result)
        val_labels.append(label)

# B∆Ø·ªöC 5: X·ª≠ l√Ω TEST set (KH√îNG augmentation)
print(f"\n[3/3] Xu ly TEST set ({len(test_df)} files)...")
print("      Augmentation: TAT")

test_spectrograms = []
test_labels = []

for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc="Test"):
    file_path = os.path.join(DATA_PATH, row['filename'])
    label = row['target']
    
    result = extract_mel_spectrogram(file_path, IMG_SIZE, augment=False)
    
    if result is not None:
        test_spectrograms.append(result)
        test_labels.append(label)

# Convert to numpy arrays
train_spectrograms = np.array(train_spectrograms)
train_labels = np.array(train_labels)
val_spectrograms = np.array(val_spectrograms)
val_labels = np.array(val_labels)
test_spectrograms = np.array(test_spectrograms)
test_labels = np.array(test_labels)

print(f"\n=> KET QUA:")
print(f"   Train: {train_spectrograms.shape[0]} samples" + (f" (augmented {len(train_df)}x6)" if APPLY_AUGMENTATION else ""))
print(f"   Val:   {val_spectrograms.shape[0]} samples (clean)")
print(f"   Test:  {test_spectrograms.shape[0]} samples (clean)")

# PyTorch format: (samples, channels, height, width)
X_train = train_spectrograms.reshape(-1, 1, IMG_SIZE, IMG_SIZE)
y_train = train_labels
X_val = val_spectrograms.reshape(-1, 1, IMG_SIZE, IMG_SIZE)
y_val = val_labels
X_test = test_spectrograms.reshape(-1, 1, IMG_SIZE, IMG_SIZE)
y_test = test_labels

# =============================================================================
# PYTORCH DATASET & DATALOADER
# =============================================================================

class AudioDataset(Dataset):
    """
    PyTorch Dataset cho audio spectrograms v·ªõi h·ªó tr·ª£ SpecAugment
    
    Dataset n√†y t·∫£i mel spectrograms v√† √°p d·ª•ng SpecAugment trong qu√° tr√¨nh training.
    SpecAugment ch·ªâ √°p d·ª•ng cho t·∫≠p train, kh√¥ng √°p d·ª•ng cho val/test.
    
    Args:
        spectrograms: M·∫£ng numpy ch·ª©a mel spectrograms, shape: (N, 1, H, W)
                      N = s·ªë l∆∞·ª£ng samples, H = W = img_size (128)
        labels: M·∫£ng numpy ch·ª©a nh√£n (0-49 cho 50 classes)
        apply_specaugment: True = √°p d·ª•ng SpecAugment (ch·ªâ d√πng cho training)
    """
    def __init__(self, spectrograms, labels, apply_specaugment=False):
        self.spectrograms = spectrograms  # Gi·ªØ d·∫°ng numpy ƒë·ªÉ augmentation
        self.labels = torch.LongTensor(labels)  # Chuy·ªÉn labels sang torch tensor
        self.apply_specaugment = apply_specaugment
        
        # Kh·ªüi t·∫°o SpecAugment n·∫øu c·∫ßn (ch·ªâ cho training)
        if apply_specaugment:
            self.spec_augment = SpecAugment(
                freq_mask_param=20,  # ƒê·ªô r·ªông t·ªëi ƒëa frequency mask (20 bins)
                time_mask_param=20,  # ƒê·ªô r·ªông t·ªëi ƒëa time mask (20 frames)
                n_freq_masks=2,      # S·ªë l∆∞·ª£ng frequency masks
                n_time_masks=2       # S·ªë l∆∞·ª£ng time masks
            )
    
    def __len__(self):
        """Tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng samples trong dataset"""
        return len(self.labels)
    
    def __getitem__(self, idx):
        """
        L·∫•y 1 sample t·ª´ dataset
        
        Args:
            idx: Index c·ªßa sample c·∫ßn l·∫•y
        
        Returns:
            spec: Mel spectrogram tensor, shape (1, 128, 128)
            label: Nh√£n (0-49)
        """
        spec = self.spectrograms[idx].copy()  # Copy ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng d·ªØ li·ªáu g·ªëc
        label = self.labels[idx]
        
        # √Åp d·ª•ng SpecAugment trong qu√° tr√¨nh training
        # M·ªói l·∫ßn l·∫•y sample s·∫Ω random mask kh√°c nhau ‚Üí tƒÉng t√≠nh ƒëa d·∫°ng
        if self.apply_specaugment:
            spec = self.spec_augment(spec)
        
        # Chuy·ªÉn sang PyTorch tensor
        spec = torch.FloatTensor(spec)
        
        return spec, label

# Create datasets with SpecAugment
train_dataset = AudioDataset(X_train, y_train, apply_specaugment=APPLY_SPECAUGMENT)
val_dataset = AudioDataset(X_val, y_val, apply_specaugment=False)  # No augment for val
test_dataset = AudioDataset(X_test, y_test, apply_specaugment=False)  # No augment for test

# Create dataloaders
BATCH_SIZE = 32
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

print(f"\n=> DataLoaders:")
print(f"   Train: {len(train_loader)} batches ({len(train_dataset)} samples)")
print(f"   Val:   {len(val_loader)} batches ({len(val_dataset)} samples)")
print(f"   Test:  {len(test_loader)} batches ({len(test_dataset)} samples)")

if APPLY_SPECAUGMENT:
    print(f"\n=> SpecAugment: ENABLED for training")
    print(f"   - Frequency masks: 2 (max width: 20 bins)")
    print(f"   - Time masks: 2 (max width: 20 frames)")
else:
    print(f"\n=> SpecAugment: DISABLED")

# =============================================================================
# X√ÇY D·ª∞NG CNN MODEL (PyTorch)
# =============================================================================

print("\n" + "="*70)
print("XAY DUNG CNN ARCHITECTURE")
print("="*70)

class AudioCNN(nn.Module):
    """
    CNN Architecture cho Audio Classification (PHI√äN B·∫¢N C·∫¢I TI·∫æN)
    
    Ki·∫øn tr√∫c: 4 Conv Blocks + 3 Fully Connected Layers
    
    Chi ti·∫øt t·ª´ng block:
        Block 1: Conv(32)‚ÜíBN‚ÜíReLU ‚Üí Conv(32)‚ÜíBN‚ÜíReLU ‚Üí MaxPool(2x2) ‚Üí Dropout(0.2)
                 Input: 1√ó128√ó128 ‚Üí Output: 32√ó64√ó64
        
        Block 2: Conv(64)‚ÜíBN‚ÜíReLU ‚Üí Conv(64)‚ÜíBN‚ÜíReLU ‚Üí MaxPool(2x2) ‚Üí Dropout(0.2)
                 Input: 32√ó64√ó64 ‚Üí Output: 64√ó32√ó32
        
        Block 3: Conv(128)‚ÜíBN‚ÜíReLU ‚Üí Conv(128)‚ÜíBN‚ÜíReLU ‚Üí MaxPool(2x2) ‚Üí Dropout(0.3)
                 Input: 64√ó32√ó32 ‚Üí Output: 128√ó16√ó16
        
        Block 4: Conv(256)‚ÜíBN‚ÜíReLU ‚Üí Conv(256)‚ÜíBN‚ÜíReLU ‚Üí AdaptiveAvgPool(1√ó1) ‚Üí Dropout(0.3)
                 Input: 128√ó16√ó16 ‚Üí Output: 256√ó1√ó1
        
        FC Layers:
                 FC(256) ‚Üí BN ‚Üí ReLU ‚Üí Dropout(0.4)
                 FC(128) ‚Üí BN ‚Üí ReLU ‚Üí Dropout(0.3)
                 FC(50) ‚Üí Logits
    
    C√°c k·ªπ thu·∫≠t ƒë∆∞·ª£c s·ª≠ d·ª•ng:
        - Batch Normalization: Chu·∫©n h√≥a d·ªØ li·ªáu gi·ªØa c√°c layers, gi√∫p train nhanh h∆°n
        - Dropout: T·∫Øt ng·∫´u nhi√™n neurons ƒë·ªÉ gi·∫£m overfitting
        - He Initialization: Kh·ªüi t·∫°o weights ph√π h·ª£p v·ªõi ReLU activation
        - Adaptive Average Pooling: ƒê·∫£m b·∫£o output size c·ªë ƒë·ªãnh b·∫•t k·ªÉ input size
    """
    def __init__(self, num_classes=50):
        super(AudioCNN, self).__init__()
        
        # ===== BLOCK 1: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng c·∫•p th·∫•p =====
        # Input: 1√ó128√ó128 (1 channel mel spectrogram)
        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)   # ‚Üí 32√ó128√ó128
        self.bn1_1 = nn.BatchNorm2d(32)                              # Chu·∫©n h√≥a
        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)  # ‚Üí 32√ó128√ó128
        self.bn1_2 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(2, 2)                              # ‚Üí 32√ó64√ó64 (gi·∫£m 1/2)
        self.dropout1 = nn.Dropout2d(0.2)                            # Dropout 20%
        
        # ===== BLOCK 2: ƒê·∫∑c tr∆∞ng c·∫•p trung =====
        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # ‚Üí 64√ó64√ó64
        self.bn2_1 = nn.BatchNorm2d(64)
        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)  # ‚Üí 64√ó64√ó64
        self.bn2_2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(2, 2)                              # ‚Üí 64√ó32√ó32
        self.dropout2 = nn.Dropout2d(0.2)
        
        # ===== BLOCK 3: ƒê·∫∑c tr∆∞ng c·∫•p cao =====
        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # ‚Üí 128√ó32√ó32
        self.bn3_1 = nn.BatchNorm2d(128)
        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)# ‚Üí 128√ó32√ó32
        self.bn3_2 = nn.BatchNorm2d(128)
        self.pool3 = nn.MaxPool2d(2, 2)                              # ‚Üí 128√ó16√ó16
        self.dropout3 = nn.Dropout2d(0.3)                            # TƒÉng dropout l√™n 30%
        
        # ===== BLOCK 4: ƒê·∫∑c tr∆∞ng semantic (√Ω nghƒ©a) =====
        self.conv4_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)# ‚Üí 256√ó16√ó16
        self.bn4_1 = nn.BatchNorm2d(256)
        self.conv4_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)# ‚Üí 256√ó16√ó16
        self.bn4_2 = nn.BatchNorm2d(256)
        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))            # ‚Üí 256√ó1√ó1 (global pooling)
        self.dropout4 = nn.Dropout2d(0.3)
        
        # ===== FULLY CONNECTED LAYERS: Ph√¢n lo·∫°i =====
        # Flatten: 256√ó1√ó1 ‚Üí 256
        self.fc1 = nn.Linear(256, 256)      # Layer 1: 256 neurons
        self.bn_fc1 = nn.BatchNorm1d(256)
        self.dropout_fc1 = nn.Dropout(0.4)  # Dropout cao (40%) ƒë·ªÉ gi·∫£m overfitting
        
        self.fc2 = nn.Linear(256, 128)      # Layer 2: 128 neurons
        self.bn_fc2 = nn.BatchNorm1d(128)
        self.dropout_fc2 = nn.Dropout(0.3)
        
        self.fc3 = nn.Linear(128, num_classes)  # Output layer: 50 classes
        
        # Kh·ªüi t·∫°o weights (He initialization cho ReLU)
        self._init_weights()
    
    def _init_weights(self):
        """
        Kh·ªüi t·∫°o weights cho c√°c layers
        
        - Conv2D: He initialization (ph√π h·ª£p v·ªõi ReLU)
        - BatchNorm: weight=1, bias=0
        - Linear: Gaussian v·ªõi mean=0, std=0.01
        """
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                # He initialization: T·ªëi ∆∞u cho ReLU activation
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):
                # BatchNorm: scale=1, shift=0
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                # Fully connected: Gaussian nh·ªè
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """
        Forward pass: Truy·ªÅn d·ªØ li·ªáu qua m·∫°ng CNN
        
        Args:
            x: Input tensor, shape (batch_size, 1, 128, 128)
        
        Returns:
            Output logits, shape (batch_size, 50)
            Ch∆∞a qua softmax - s·∫Ω d√πng CrossEntropyLoss t√≠nh to√°n
        """
        # ===== BLOCK 1: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng c·∫•p th·∫•p =====
        # H·ªçc c√°c patterns ƒë∆°n gi·∫£n: edges, textures
        x = F.relu(self.bn1_1(self.conv1_1(x)))  # Conv ‚Üí BN ‚Üí ReLU
        x = F.relu(self.bn1_2(self.conv1_2(x)))
        x = self.pool1(x)                         # Gi·∫£m k√≠ch th∆∞·ªõc 1/2
        x = self.dropout1(x)                      # Regularization
        
        # ===== BLOCK 2: ƒê·∫∑c tr∆∞ng c·∫•p trung =====
        # H·ªçc c√°c patterns ph·ª©c t·∫°p h∆°n: shapes, patterns
        x = F.relu(self.bn2_1(self.conv2_1(x)))
        x = F.relu(self.bn2_2(self.conv2_2(x)))
        x = self.pool2(x)
        x = self.dropout2(x)
        
        # ===== BLOCK 3: ƒê·∫∑c tr∆∞ng c·∫•p cao =====
        # H·ªçc c√°c patterns tr·ª´u t∆∞·ª£ng: objects, structures
        x = F.relu(self.bn3_1(self.conv3_1(x)))
        x = F.relu(self.bn3_2(self.conv3_2(x)))
        x = self.pool3(x)
        x = self.dropout3(x)
        
        # ===== BLOCK 4: ƒê·∫∑c tr∆∞ng semantic =====
        # H·ªçc √Ω nghƒ©a cao nh·∫•t: concepts, semantics
        x = F.relu(self.bn4_1(self.conv4_1(x)))
        x = F.relu(self.bn4_2(self.conv4_2(x)))
        x = self.adaptive_pool(x)  # Global pooling ‚Üí 1√ó1
        x = self.dropout4(x)
        
        # ===== FLATTEN =====
        # Chuy·ªÉn t·ª´ 4D (batch, channels, height, width) ‚Üí 2D (batch, features)
        x = x.view(x.size(0), -1)  # (batch, 256, 1, 1) ‚Üí (batch, 256)
        
        # ===== FULLY CONNECTED LAYERS: Ph√¢n lo·∫°i =====
        # FC1: 256 ‚Üí 256
        x = self.fc1(x)
        x = self.bn_fc1(x)
        x = F.relu(x)
        x = self.dropout_fc1(x)
        
        # FC2: 256 ‚Üí 128
        x = self.fc2(x)
        x = self.bn_fc2(x)
        x = F.relu(x)
        x = self.dropout_fc2(x)
        
        # FC3: 128 ‚Üí 50 (output layer)
        x = self.fc3(x)  # Logits (ch∆∞a qua softmax)
        
        return x

# Build model
model = AudioCNN(num_classes=50).to(DEVICE)

# Model summary
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print("\nModel Architecture:")
print(model)
print(f"\nTrainable parameters: {count_parameters(model):,}")

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # L2 regularization
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)

# =============================================================================
# TRAINING FUNCTIONS
# =============================================================================

def train_epoch(model, loader, criterion, optimizer, device):
    """
    Hu·∫•n luy·ªán model trong 1 epoch
    
    Args:
        model: M√¥ h√¨nh CNN c·∫ßn train
        loader: DataLoader ch·ª©a d·ªØ li·ªáu training
        criterion: Loss function (CrossEntropyLoss)
        optimizer: Optimizer (Adam)
        device: 'cuda' ho·∫∑c 'cpu'
    
    Returns:
        avg_loss: Loss trung b√¨nh c·ªßa epoch
        accuracy: ƒê·ªô ch√≠nh x√°c (%) tr√™n t·∫≠p train
    """
    model.train()  # B·∫≠t training mode (dropout, batchnorm ho·∫°t ƒë·ªông)
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(loader, desc='Training')
    for inputs, labels in pbar:
        # Chuy·ªÉn d·ªØ li·ªáu sang GPU/CPU
        inputs, labels = inputs.to(device), labels.to(device)
        
        # B∆∞·ªõc 1: X√≥a gradients c≈©
        optimizer.zero_grad()
        
        # B∆∞·ªõc 2: Forward pass - t√≠nh output
        outputs = model(inputs)
        
        # B∆∞·ªõc 3: T√≠nh loss
        loss = criterion(outputs, labels)
        
        # B∆∞·ªõc 4: Backward pass - t√≠nh gradients
        loss.backward()
        
        # B∆∞·ªõc 5: Gradient clipping - tr√°nh exploding gradients
        # Gi·ªõi h·∫°n norm c·ªßa gradients ‚â§ 1.0
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # B∆∞·ªõc 6: C·∫≠p nh·∫≠t weights
        optimizer.step()
        
        # Th·ªëng k√™
        running_loss += loss.item()
        _, predicted = outputs.max(1)  # L·∫•y class c√≥ x√°c su·∫•t cao nh·∫•t
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        # Hi·ªÉn th·ªã progress bar v·ªõi loss v√† accuracy hi·ªán t·∫°i
        pbar.set_postfix({'loss': running_loss/len(loader), 'acc': 100.*correct/total})
    
    return running_loss / len(loader), 100. * correct / total

def validate(model, loader, criterion, device):
    """
    ƒê√°nh gi√° model tr√™n t·∫≠p validation/test
    
    Args:
        model: M√¥ h√¨nh CNN c·∫ßn ƒë√°nh gi√°
        loader: DataLoader ch·ª©a d·ªØ li·ªáu val/test
        criterion: Loss function
        device: 'cuda' ho·∫∑c 'cpu'
    
    Returns:
        avg_loss: Loss trung b√¨nh
        accuracy: ƒê·ªô ch√≠nh x√°c (%)
    """
    model.eval()  # B·∫≠t evaluation mode (dropout t·∫Øt, batchnorm d√πng running stats)
    running_loss = 0.0
    correct = 0
    total = 0
    
    # Kh√¥ng t√≠nh gradients (ti·∫øt ki·ªám memory v√† tƒÉng t·ªëc)
    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            
            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            # Th·ªëng k√™
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    return running_loss / len(loader), 100. * correct / total

# =============================================================================
# TRAINING LOOP
# =============================================================================

print("\n" + "="*70)
print("HUAN LUYEN CNN MODEL")
print("="*70)

EPOCHS = 100
best_val_acc = 0
best_val_loss = float('inf')
patience = 20
patience_counter = 0

train_losses = []
train_accs = []
val_losses = []
val_accs = []

print(f"\nHyperparameters:")
print(f"  - Epochs: {EPOCHS}")
print(f"  - Batch size: {BATCH_SIZE}")
print(f"  - Optimizer: Adam (lr=0.001, weight_decay=1e-4)")
print(f"  - Early stopping patience: {patience}")
print(f"  - LR scheduler: ReduceLROnPlateau (factor=0.5, patience=10)")

for epoch in range(EPOCHS):
    print(f"\nEpoch {epoch+1}/{EPOCHS}")
    print("-" * 70)
    
    # Train
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)
    
    # Validate
    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)
    
    # Learning rate scheduling
    old_lr = optimizer.param_groups[0]['lr']
    scheduler.step(val_loss)
    new_lr = optimizer.param_groups[0]['lr']
    
    # Print if learning rate changed
    if old_lr != new_lr:
        print(f"‚ö° Learning Rate changed: {old_lr:.6f} ‚Üí {new_lr:.6f}")
    
    # Store history
    train_losses.append(train_loss)
    train_accs.append(train_acc)
    val_losses.append(val_loss)
    val_accs.append(val_acc)
    
    print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
    print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")
    
    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_cnn_improved_model.pth')
        print(f"‚úì Model saved! (Val Acc: {val_acc:.2f}%)")
        patience_counter = 0
    else:
        patience_counter += 1
    
    # Early stopping
    if patience_counter >= patience:
        print(f"\nEarly stopping triggered after {epoch+1} epochs")
        break

print(f"\n=> Training completed!")
print(f"   Best validation accuracy: {best_val_acc:.2f}%")

# =============================================================================
# ƒê√ÅNH GI√Å
# =============================================================================

print("\n" + "="*70)
print("DANH GIA MODEL")
print("="*70)

# Load best model
model.load_state_dict(torch.load('best_cnn_improved_model.pth'))
model.eval()

# Predict on test set
all_preds = []
all_labels = []

with torch.no_grad():
    for inputs, labels in tqdm(test_loader, desc='Testing'):
        inputs = inputs.to(DEVICE)
        outputs = model(inputs)
        _, predicted = outputs.max(1)
        
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.numpy())

y_pred = np.array(all_preds)
y_true = np.array(all_labels)

# Accuracy
accuracy = accuracy_score(y_true, y_pred)
print(f"\n=> ACCURACY: {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"   Dung: {np.sum(y_pred == y_true)}/{len(y_true)}")

# Classification report
print("\nBao cao chi tiet:")
print(classification_report(y_true, y_pred))

# Confusion matrix
plt.figure(figsize=(12, 10))
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', cbar=True)
plt.title(f'Confusion Matrix - CNN Model (Accuracy: {accuracy:.2%})')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig('confusion_matrix_CNN_improved.png', dpi=300)
plt.close()
print("\n=> Da luu: confusion_matrix_CNN_improved.png")

# =============================================================================
# TRAINING HISTORY
# =============================================================================

print("\n" + "="*70)
print("TRAINING HISTORY")
print("="*70)

# Plot accuracy
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(train_accs, label='Train Accuracy', linewidth=2)
plt.plot(val_accs, label='Val Accuracy', linewidth=2)
plt.title('Model Accuracy', fontsize=14, fontweight='bold')
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Accuracy (%)', fontsize=12)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(train_losses, label='Train Loss', linewidth=2)
plt.plot(val_losses, label='Val Loss', linewidth=2)
plt.title('Model Loss', fontsize=14, fontweight='bold')
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_history_CNN_improved.png', dpi=300)
plt.close()
print("=> Da luu: training_history_CNN_improved.png")

# Best epoch info
best_epoch_idx = np.argmax(val_accs)
print(f"\nBest Epoch: {best_epoch_idx + 1}")
print(f"Best Val Accuracy: {val_accs[best_epoch_idx]:.2f}%")

# =============================================================================
# SO S√ÅNH V·ªöI TRADITIONAL ML
# =============================================================================

print("\n" + "="*70)
print("SO SANH VOI TRADITIONAL ML")
print("="*70)

trainable_params = count_parameters(model)

comparison = {
    'Model': ['SVM (Traditional ML)', 'CNN (Deep Learning)'],
    'Accuracy': [0.7625, accuracy],
    'Training Time': ['~2 ph√∫t', f'~{len(train_losses)} epochs'],
    'Parameters': ['~200 features', f'{trainable_params:,}'],
    'Input Type': ['Handcrafted features', 'Raw spectrogram']
}

comparison_df = pd.DataFrame(comparison)
print(comparison_df.to_string(index=False))

improvement = (accuracy - 0.7625) / 0.7625 * 100
print(f"\n=> Cai thien: +{improvement:.2f}%")
print(f"   Tu 76.25% -> {accuracy*100:.2f}%")

if accuracy >= 0.85:
    print(f"\nüéâ M·ª§C TI√äU ƒê·∫†T ƒê∆Ø·ª¢C: 85%+!")
elif accuracy >= 0.80:
    print(f"\n‚úÖ T·ªët! G·∫ßn ƒë·∫°t m·ª•c ti√™u 85%")
else:
    print(f"\n‚ö†Ô∏è C·∫ßn th√™m augmentation ho·∫∑c train l√¢u h∆°n")

# =============================================================================
# VISUALIZE PREDICTIONS
# =============================================================================

print("\n" + "="*70)
print("VISUALIZE PREDICTIONS")
print("="*70)

# L·∫•y category names
df_loaded = pd.read_csv(CSV_PATH)
target_to_category = dict(zip(df_loaded['target'], df_loaded['category']))

# Ch·ªçn random 9 samples
np.random.seed(42)
indices = np.random.choice(len(X_test), 9, replace=False)

plt.figure(figsize=(15, 10))
for i, idx in enumerate(indices):
    plt.subplot(3, 3, i + 1)
    
    # Hi·ªÉn th·ªã spectrogram (convert t·ª´ CHW -> HW)
    spec_img = X_test[idx].squeeze()  # Remove channel dimension
    plt.imshow(spec_img, cmap='viridis', aspect='auto')
    
    # True vs Predicted
    true_label = y_test[idx]
    pred_label = y_pred[idx]
    true_category = target_to_category[true_label]
    pred_category = target_to_category[pred_label]
    
    color = 'green' if true_label == pred_label else 'red'
    plt.title(f'True: {true_category}\nPred: {pred_category}', color=color, fontsize=9)
    plt.axis('off')

plt.tight_layout()
plt.savefig('predictions_CNN.png', dpi=300)
plt.close()
print("=> Da luu: predictions_CNN.png")

# =============================================================================
# SAVE MODEL INFO
# =============================================================================

print("\n" + "="*70)
print("HOAN TAT!")
print("="*70)

model_info = f"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
CNN MODEL FOR AUDIO CLASSIFICATION (PyTorch)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

DATASET:
  - ESC-50: {len(df)} samples, 50 classes
  - Input: Mel Spectrogram ({IMG_SIZE}x{IMG_SIZE})
  - Split: Train {len(train_dataset)} | Val {len(val_dataset)} | Test {len(test_dataset)}
  - Augmentation: {'YES (6x for training only)' if APPLY_AUGMENTATION else 'NO'}
  - Device: {DEVICE}

MODEL ARCHITECTURE:
  - Type: Custom CNN (4 conv blocks + 3 FC layers)
  - Parameters: {trainable_params:,}
  - Regularization: Dropout (0.2-0.4) + BatchNorm + L2 (1e-4)

TRAINING:
  - Epochs: {len(train_losses)} / {EPOCHS}
  - Best epoch: {best_epoch_idx + 1}
  - Batch size: {BATCH_SIZE}
  - Optimizer: Adam (lr=0.001, weight_decay=1e-4)
  - LR Scheduler: ReduceLROnPlateau
  - Early stopping: patience={patience}

RESULTS:
  - Train Accuracy: {train_accs[best_epoch_idx]:.2f}%
  - Val Accuracy: {best_val_acc:.2f}%
  - Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)
  - Baseline (SVM): 76.25%
  - Improvement: {improvement:+.2f}%

OUTPUT FILES:
  ‚úì best_cnn_improved_model.pth (trained weights)
  ‚úì confusion_matrix_CNN_improved.png
  ‚úì training_history_CNN_improved.png
  ‚úì predictions_CNN.png
  ‚úì cnn_model_info.txt

USAGE:
  from cnn_model import AudioCNN
  import torch
  
  model = AudioCNN(num_classes=50)
  model.load_state_dict(torch.load('best_cnn_improved_model.pth'))
  model.eval()

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""

with open('cnn_model_info.txt', 'w', encoding='utf-8') as f:
    f.write(model_info)

print(model_info)
print("=> Da luu: cnn_model_info.txt")
print("="*70)

# =============================================================================
# TR·ª∞C QUAN MEL SPECTROGRAMS
# =============================================================================

print("\n" + "="*70)
print("TRUC QUAN MEL SPECTROGRAMS")
print("="*70)

# T·∫°o th∆∞ m·ª•c
os.makedirs('mel_spectrograms', exist_ok=True)

# L·∫•y m·ªôt s·ªë samples ng·∫´u nhi√™n t·ª´ c√°c class kh√°c nhau
np.random.seed(42)
n_samples = 8
sample_indices = np.random.choice(len(df), n_samples, replace=False)

fig, axes = plt.subplots(2, 4, figsize=(20, 10))
axes = axes.ravel()

print(f"Dang tao {n_samples} mel spectrogram visualizations...")

for idx, sample_idx in enumerate(sample_indices):
    row = df.iloc[sample_idx]
    file_path = os.path.join(DATA_PATH, row['filename'])
    label = row['target']
    category = row['category']
    
    # Load audio
    y, sr = librosa.load(file_path, sr=22050, duration=5.0)
    
    # T·∫°o mel spectrogram (kh√¥ng normalize ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp)
    mel_spec = librosa.feature.melspectrogram(
        y=y, sr=sr, n_mels=128, n_fft=2048, hop_length=512, fmax=8000
    )
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    
    # Plot
    img = librosa.display.specshow(
        mel_spec_db, 
        sr=sr, 
        hop_length=512,
        x_axis='time', 
        y_axis='mel',
        ax=axes[idx],
        cmap='viridis'
    )
    
    axes[idx].set_title(f'{category}\n(Class {label})', fontsize=11, pad=10)
    axes[idx].set_xlabel('Time (s)', fontsize=9)
    axes[idx].set_ylabel('Frequency (Hz)', fontsize=9)

plt.suptitle('Mel Spectrogram Examples - Audio "Images"', fontsize=16, y=0.98)
plt.tight_layout()
plt.savefig('mel_spectrograms/mel_spectrogram_examples.png', dpi=150, bbox_inches='tight')
print("=> Da luu: mel_spectrograms/mel_spectrogram_examples.png")

# T·∫°o visualization chi ti·∫øt cho 1 sample
print("\nTao visualization chi tiet cho 1 sample...")
sample_row = df.iloc[0]
file_path = os.path.join(DATA_PATH, sample_row['filename'])
category = sample_row['category']

y, sr = librosa.load(file_path, sr=22050, duration=5.0)

fig, axes = plt.subplots(3, 1, figsize=(14, 10))

# 1. Waveform
axes[0].plot(np.linspace(0, len(y)/sr, len(y)), y, linewidth=0.5, color='blue')
axes[0].set_title(f'1. Audio Waveform - "{category}"', fontsize=12, pad=10)
axes[0].set_xlabel('Time (s)')
axes[0].set_ylabel('Amplitude')
axes[0].grid(True, alpha=0.3)

# 2. Mel Spectrogram (dB)
mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)
mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
img = librosa.display.specshow(
    mel_spec_db, sr=sr, x_axis='time', y_axis='mel', ax=axes[1], cmap='viridis'
)
axes[1].set_title('2. Mel Spectrogram (dB) - "·∫¢nh" c·ªßa √¢m thanh', fontsize=12, pad=10)
axes[1].set_xlabel('Time (s)')
axes[1].set_ylabel('Frequency (Hz)')
fig.colorbar(img, ax=axes[1], format='%+2.0f dB')

# 3. Mel Spectrogram normalized (input cho CNN)
mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min())
mel_spec_resized = cv2.resize(mel_spec_norm, (128, 128))
img2 = axes[2].imshow(mel_spec_resized, aspect='auto', origin='lower', cmap='viridis')
axes[2].set_title('3. Normalized Mel Spectrogram (128√ó128) - Input cho CNN', fontsize=12, pad=10)
axes[2].set_xlabel('Time bins')
axes[2].set_ylabel('Mel frequency bins')
fig.colorbar(img2, ax=axes[2], label='Normalized value [0-1]')

plt.tight_layout()
plt.savefig('mel_spectrograms/mel_spectrogram_detailed.png', dpi=150, bbox_inches='tight')
print("=> Da luu: mel_spectrograms/mel_spectrogram_detailed.png")

# So s√°nh c√°c class kh√°c nhau
print("\nTao so sanh mel spectrograms cua cac class...")
classes_to_compare = ['dog', 'cat', 'rooster', 'rain', 'thunderstorm', 'crackling_fire']
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.ravel()

for idx, class_name in enumerate(classes_to_compare):
    # T√¨m sample c·ªßa class n√†y
    sample = df[df['category'] == class_name].iloc[0] if class_name in df['category'].values else None
    
    if sample is not None:
        file_path = os.path.join(DATA_PATH, sample['filename'])
        y, sr = librosa.load(file_path, sr=22050, duration=5.0)
        
        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
        
        librosa.display.specshow(
            mel_spec_db, sr=sr, x_axis='time', y_axis='mel',
            ax=axes[idx], cmap='viridis'
        )
        axes[idx].set_title(f'"{class_name}"', fontsize=12, pad=10, fontweight='bold')
        axes[idx].set_xlabel('Time (s)', fontsize=9)
        axes[idx].set_ylabel('Freq (Hz)', fontsize=9)
    else:
        axes[idx].text(0.5, 0.5, 'Not found', ha='center', va='center')
        axes[idx].set_title(f'"{class_name}" (not found)', fontsize=10)

plt.suptitle('So s√°nh Mel Spectrograms - M·ªói class c√≥ "visual signature" ri√™ng', 
             fontsize=14, y=0.98, fontweight='bold')
plt.tight_layout()
plt.savefig('mel_spectrograms/mel_spectrogram_comparison.png', dpi=150, bbox_inches='tight')
print("=> Da luu: mel_spectrograms/mel_spectrogram_comparison.png")

print("\n=> HO√ÄN T·∫§T! Ki·ªÉm tra folder 'mel_spectrograms/' ƒë·ªÉ xem:")
print("   - mel_spectrogram_examples.png (8 samples ng·∫´u nhi√™n)")
print("   - mel_spectrogram_detailed.png (chi ti·∫øt 1 sample)")
print("   - mel_spectrogram_comparison.png (so s√°nh c√°c class)")

print("\n" + "="*70)
print("üöÄ HO√ÄN T·∫§T! Ki·ªÉm tra c√°c file output:")
print("   - best_cnn_model.pth")
print("   - confusion_matrix_CNN.png")
print("   - training_history_CNN.png") 
print("   - predictions_CNN.png")
print("   - cnn_model_info.txt")
print("   - mel_spectrograms/ (folder ch·ª©a visualizations)")
print("="*70)

