# -*- coding: utf-8 -*-
"""
TRANSFER LEARNING - RESNET18 FOR AUDIO CLASSIFICATION
Sá»­ dá»¥ng pretrained ResNet18 tá»« ImageNet, fine-tune cho ESC-50
Má»¥c tiÃªu: 88-92% accuracy
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from tqdm import tqdm

# PyTorch
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.models as models

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

# =============================================================================
# Cáº¤U HÃŒNH
# =============================================================================

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
BATCH_SIZE = 32
EPOCHS = 80
LEARNING_RATE = 0.001
EARLY_STOPPING_PATIENCE = 15

print("="*80)
print("TRANSFER LEARNING - RESNET18 CHO PHÃ‚N LOáº I Ã‚M THANH")
print("="*80)
print(f"PhiÃªn báº£n PyTorch: {torch.__version__}")
print(f"Thiáº¿t bá»‹: {DEVICE}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
print("="*80)

# =============================================================================
# SPECAUGMENT CLASS
# =============================================================================

class SpecAugment:
    """
    SpecAugment: Frequency & Time Masking cho regularization
    
    Ká»¹ thuáº­t augmentation chuyÃªn cho audio, che ngáº«u nhiÃªn cÃ¡c dáº£i táº§n sá»‘ vÃ  thá»i gian.
    GiÃºp model há»c robust hÆ¡n, khÃ´ng bá»‹ phá»¥ thuá»™c vÃ o má»™t vÃ i táº§n sá»‘/thá»i Ä‘iá»ƒm cá»¥ thá»ƒ.
    """
    def __init__(self, freq_mask_param=25, time_mask_param=25, n_freq_masks=2, n_time_masks=2):
        """
        Khá»Ÿi táº¡o SpecAugment
        
        Args:
            freq_mask_param: Äá»™ rá»™ng tá»‘i Ä‘a cá»§a frequency mask (25 bins)
            time_mask_param: Äá»™ rá»™ng tá»‘i Ä‘a cá»§a time mask (25 frames)
            n_freq_masks: Sá»‘ lÆ°á»£ng frequency masks (2)
            n_time_masks: Sá»‘ lÆ°á»£ng time masks (2)
        """
        self.freq_mask_param = freq_mask_param
        self.time_mask_param = time_mask_param
        self.n_freq_masks = n_freq_masks
        self.n_time_masks = n_time_masks
    
    def __call__(self, spec):
        """
        Ãp dá»¥ng SpecAugment lÃªn mel spectrogram
        
        Args:
            spec: Mel spectrogram, shape (H, W) hoáº·c (1, H, W)
        
        Returns:
            Augmented spectrogram vá»›i cÃ¡c vÃ¹ng bá»‹ mask = 0
        """
        spec = spec.copy()
        
        # Xá»­ lÃ½ cáº£ Ä‘á»‹nh dáº¡ng 2D vÃ  3D
        if len(spec.shape) == 3:
            spec = spec.squeeze(0)
            had_channel = True
        else:
            had_channel = False
        
        num_freq_bins, num_time_frames = spec.shape
        
        # Frequency masking: Che cÃ¡c dáº£i táº§n sá»‘ ngáº«u nhiÃªn
        # Giá»›i háº¡n Ä‘á»™ rá»™ng mask â‰¤ 1/4 tá»•ng sá»‘ bins Ä‘á»ƒ khÃ´ng máº¥t quÃ¡ nhiá»u thÃ´ng tin
        for _ in range(self.n_freq_masks):
            f = np.random.randint(0, min(self.freq_mask_param, num_freq_bins // 4))
            f0 = np.random.randint(0, num_freq_bins - f)
            spec[f0:f0 + f, :] = 0
        
        # Time masking: Che cÃ¡c khung thá»i gian ngáº«u nhiÃªn
        for _ in range(self.n_time_masks):
            t = np.random.randint(0, min(self.time_mask_param, num_time_frames // 4))
            t0 = np.random.randint(0, num_time_frames - t)
            spec[:, t0:t0 + t] = 0
        
        # KhÃ´i phá»¥c chiá»u channel náº¿u cáº§n
        if had_channel:
            spec = np.expand_dims(spec, axis=0)
        
        return spec

# =============================================================================
# DATASET CLASS
# =============================================================================

class AudioDataset(Dataset):
    """
    PyTorch Dataset cho audio spectrograms vá»›i SpecAugment
    
    Äáº·c biá»‡t: Chuyá»ƒn Ä‘á»•i 1-channel (grayscale) â†’ 3-channel (RGB) Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i ResNet
    ResNet Ä‘Æ°á»£c pretrain trÃªn ImageNet (áº£nh RGB), nÃªn cáº§n input 3 channels.
    """
    def __init__(self, spectrograms, labels, apply_specaugment=False):
        """
        Khá»Ÿi táº¡o Dataset
        
        Args:
            spectrograms: Máº£ng numpy chá»©a mel spectrograms, shape (N, 1, H, W)
            labels: Máº£ng numpy chá»©a nhÃ£n (0-49)
            apply_specaugment: True = Ã¡p dá»¥ng SpecAugment (chá»‰ cho training)
        """
        self.spectrograms = spectrograms
        self.labels = torch.LongTensor(labels)
        self.apply_specaugment = apply_specaugment
        
        # Khá»Ÿi táº¡o SpecAugment náº¿u cáº§n
        if apply_specaugment:
            self.spec_augment = SpecAugment(
                freq_mask_param=25,   # TÄƒng lÃªn 25 so vá»›i CNN (20)
                time_mask_param=25,   # ResNet máº¡nh hÆ¡n, chá»‹u Ä‘Æ°á»£c augment máº¡nh hÆ¡n
                n_freq_masks=2,
                n_time_masks=2
            )
    
    def __len__(self):
        """Tráº£ vá» sá»‘ lÆ°á»£ng samples"""
        return len(self.labels)
    
    def __getitem__(self, idx):
        """
        Láº¥y 1 sample tá»« dataset
        
        Args:
            idx: Index cá»§a sample
        
        Returns:
            spec: Tensor 3-channel, shape (3, 128, 128) - tÆ°Æ¡ng thÃ­ch vá»›i ResNet
            label: NhÃ£n (0-49)
        """
        spec = self.spectrograms[idx].copy()
        label = self.labels[idx]
        
        # Ãp dá»¥ng SpecAugment náº¿u Ä‘ang training
        if self.apply_specaugment:
            spec = self.spec_augment(spec)
        
        # Chuyá»ƒn 1-channel â†’ 3-channel Ä‘á»ƒ ResNet cÃ³ thá»ƒ xá»­ lÃ½
        # ResNet pretrain trÃªn ImageNet (RGB), cáº§n 3 channels input
        if len(spec.shape) == 3 and spec.shape[0] == 1:
            # (1, H, W) â†’ (3, H, W): NhÃ¢n Ä‘Ã´i channel grayscale thÃ nh RGB
            spec = np.repeat(spec, 3, axis=0)
        elif len(spec.shape) == 2:
            # (H, W) â†’ (3, H, W): Stack 3 láº§n
            spec = np.stack([spec, spec, spec], axis=0)
        
        # Chuyá»ƒn sang PyTorch tensor
        spec = torch.FloatTensor(spec)
        
        return spec, label

# =============================================================================
# RESNET18 MODEL
# =============================================================================

class AudioResNet18(nn.Module):
    """
    ResNet18 cho phÃ¢n loáº¡i Ã¢m thanh (Transfer Learning tá»« ImageNet)
    
    Ã tÆ°á»Ÿng chÃ­nh:
    - Sá»­ dá»¥ng ResNet18 Ä‘Ã£ pretrain trÃªn ImageNet (1.2M áº£nh, 1000 classes)
    - ResNet Ä‘Ã£ há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng tá»•ng quÃ¡t (edges, textures, shapes)
    - Fine-tune láº¡i cho bÃ i toÃ¡n phÃ¢n loáº¡i Ã¢m thanh (50 classes)
    
    Æ¯u Ä‘iá»ƒm so vá»›i CNN tá»« Ä‘áº§u:
    - KhÃ´ng cáº§n train tá»« Ä‘áº§u (tiáº¿t kiá»‡m thá»i gian)
    - Táº­n dá»¥ng kiáº¿n thá»©c tá»« ImageNet (transfer learning)
    - Accuracy cao hÆ¡n vá»›i Ã­t dá»¯ liá»‡u hÆ¡n
    
    Kiáº¿n trÃºc:
    - Backbone: ResNet18 (pretrained) - 11.2M parameters
    - FC layer: Thay Ä‘á»•i tá»« 1000 classes â†’ 50 classes
    - Dropout: 0.5 Ä‘á»ƒ giáº£m overfitting
    """
    def __init__(self, num_classes=50, pretrained=True, dropout_rate=0.5):
        """
        Khá»Ÿi táº¡o AudioResNet18
        
        Args:
            num_classes: Sá»‘ lÆ°á»£ng classes cáº§n phÃ¢n loáº¡i (50 cho ESC-50)
            pretrained: True = load weights tá»« ImageNet
            dropout_rate: Tá»· lá»‡ dropout (0.5 = 50% neurons bá»‹ táº¯t)
        """
        super(AudioResNet18, self).__init__()
        
        # BÆ°á»›c 1: Load ResNet18 pretrained tá»« ImageNet
        # Táº£i toÃ n bá»™ kiáº¿n trÃºc vÃ  weights Ä‘Ã£ train sáºµn
        self.resnet = models.resnet18(pretrained=pretrained)
        
        if pretrained:
            print("\nâœ“ ÄÃ£ táº£i ResNet18 pretrained tá»« ImageNet")
        
        # BÆ°á»›c 2: Thay Ä‘á»•i FC layer cuá»‘i
        # ResNet18 gá»‘c: FC(512 â†’ 1000) cho ImageNet
        # Cáº§n thay thÃ nh: FC(512 â†’ 50) cho ESC-50
        num_features = self.resnet.fc.in_features  # 512
        self.resnet.fc = nn.Sequential(
            nn.Dropout(dropout_rate),              # Dropout Ä‘á»ƒ giáº£m overfitting
            nn.Linear(num_features, num_classes)   # 512 â†’ 50
        )
        
        # BÆ°á»›c 3: Khá»Ÿi táº¡o weights cho FC layer má»›i
        # DÃ¹ng Xavier initialization (tá»‘t cho linear layers)
        nn.init.xavier_uniform_(self.resnet.fc[1].weight)
        nn.init.constant_(self.resnet.fc[1].bias, 0)
    
    def forward(self, x):
        """
        Forward pass
        
        Args:
            x: Input tensor, shape (batch_size, 3, 128, 128)
        
        Returns:
            Logits, shape (batch_size, 50)
        """
        return self.resnet(x)
    
    def freeze_backbone(self):
        """
        ÄÃ³ng bÄƒng backbone (chá»‰ train FC layer)
        
        DÃ¹ng trong Stage 1:
        - Giá»¯ nguyÃªn weights pretrained cá»§a ResNet
        - Chá»‰ train FC layer má»›i
        - Nhanh hÆ¡n, á»•n Ä‘á»‹nh hÆ¡n
        """
        for name, param in self.resnet.named_parameters():
            if 'fc' not in name:  # Táº¥t cáº£ layers trá»« FC
                param.requires_grad = False  # KhÃ´ng tÃ­nh gradients
        print("âœ“ ÄÃ£ Ä‘Ã³ng bÄƒng backbone (chá»‰ train FC layer)")
    
    def unfreeze_backbone(self):
        """
        Má»Ÿ bÄƒng toÃ n bá»™ model (fine-tune táº¥t cáº£)
        
        DÃ¹ng trong Stage 2:
        - Train toÃ n bá»™ ResNet (backbone + FC)
        - Learning rate tháº¥p hÆ¡n Ä‘á»ƒ khÃ´ng phÃ¡ há»ng weights pretrained
        - Accuracy cao hÆ¡n nhÆ°ng cáº§n cáº©n tháº­n vá»›i overfitting
        """
        for param in self.resnet.parameters():
            param.requires_grad = True  # Táº¥t cáº£ layers Ä‘á»u train Ä‘Æ°á»£c
        print("âœ“ ÄÃ£ má»Ÿ bÄƒng backbone (train toÃ n bá»™ model)")

# =============================================================================
# LOAD DATA
# =============================================================================

print("\n" + "="*80)
print("LOADING PREPROCESSED DATA")
print("="*80)

# Load preprocessed data from previous run
try:
    # Try to load from numpy files (faster)
    X_train = np.load('cache/X_train.npy')
    y_train = np.load('cache/y_train.npy')
    X_val = np.load('cache/X_val.npy')
    y_val = np.load('cache/y_val.npy')
    X_test = np.load('cache/X_test.npy')
    y_test = np.load('cache/y_test.npy')
    
    print("âœ“ Loaded cached data from numpy files")
    
except FileNotFoundError:
    print("Cache not found. Please run cnn_model.py first to generate preprocessed data.")
    print("Or I'll extract features now...")
    
    # Import extraction functions from cnn_model
    import sys
    sys.path.insert(0, os.path.dirname(__file__))
    
    from cnn_model import extract_mel_spectrogram, create_mel_spectrogram
    import librosa
    import cv2
    
    DATA_PATH = 'data/audio/audio/'
    CSV_PATH = 'data/esc50.csv'
    IMG_SIZE = 128
    
    df = pd.read_csv(CSV_PATH)
    print(f"Dataset: {len(df)} samples, {df['target'].nunique()} classes")
    
    # Split data
    train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])
    train_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=42, stratify=train_val_df['target'])
    
    print(f"\n=> Split:")
    print(f"   Train: {len(train_df)} files")
    print(f"   Val:   {len(val_df)} files")
    print(f"   Test:  {len(test_df)} files")
    
    # HÃ m trÃ­ch xuáº¥t features tá»« dataframe
    def extract_features(df, augment=False):
        """
        TrÃ­ch xuáº¥t mel spectrograms tá»« danh sÃ¡ch audio files
        
        Args:
            df: DataFrame chá»©a thÃ´ng tin audio files
            augment: True = Ã¡p dá»¥ng data augmentation (6x samples)
        
        Returns:
            spectrograms: Máº£ng numpy chá»©a mel spectrograms
            labels: Máº£ng numpy chá»©a nhÃ£n tÆ°Æ¡ng á»©ng
        """
        spectrograms = []
        labels = []
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="Extracting"):
            file_path = os.path.join(DATA_PATH, row['filename'])
            label = row['target']
            
            # TrÃ­ch xuáº¥t mel spectrogram (cÃ³ thá»ƒ cÃ³ augmentation)
            result = extract_mel_spectrogram(file_path, IMG_SIZE, augment=augment)
            
            if result is not None:
                if augment:
                    # Náº¿u cÃ³ augmentation, result lÃ  list 6 spectrograms
                    for spec in result:
                        spectrograms.append(spec)
                        labels.append(label)
                else:
                    # KhÃ´ng augment, chá»‰ cÃ³ 1 spectrogram
                    spectrograms.append(result)
                    labels.append(label)
        
        return np.array(spectrograms), np.array(labels)
    
    # TrÃ­ch xuáº¥t features cho táº­p TRAIN (cÃ³ augmentation)
    print("\n[1/3] Extracting TRAIN features (with augmentation)...")
    train_specs, y_train = extract_features(train_df, augment=True)
    X_train = train_specs.reshape(-1, 1, IMG_SIZE, IMG_SIZE)  # Reshape vá» format PyTorch: (N, C, H, W)
    
    # TrÃ­ch xuáº¥t features cho táº­p VAL (khÃ´ng augmentation)
    print("\n[2/3] Extracting VAL features...")
    val_specs, y_val = extract_features(val_df, augment=False)
    X_val = val_specs.reshape(-1, 1, IMG_SIZE, IMG_SIZE)
    
    # TrÃ­ch xuáº¥t features cho táº­p TEST (khÃ´ng augmentation)
    print("\n[3/3] Extracting TEST features...")
    test_specs, y_test = extract_features(test_df, augment=False)
    X_test = test_specs.reshape(-1, 1, IMG_SIZE, IMG_SIZE)
    
    # LÆ°u vÃ o cache Ä‘á»ƒ láº§n sau khÃ´ng pháº£i trÃ­ch xuáº¥t láº¡i
    os.makedirs('cache', exist_ok=True)
    np.save('cache/X_train.npy', X_train)
    np.save('cache/y_train.npy', y_train)
    np.save('cache/X_val.npy', X_val)
    np.save('cache/y_val.npy', y_val)
    np.save('cache/X_test.npy', X_test)
    np.save('cache/y_test.npy', y_test)
    print("\nâœ“ Cached data saved to cache/ directory")

print(f"\n=> Data shapes:")
print(f"   Train: {X_train.shape} | Labels: {y_train.shape}")
print(f"   Val:   {X_val.shape} | Labels: {y_val.shape}")
print(f"   Test:  {X_test.shape} | Labels: {y_test.shape}")

# =============================================================================
# CREATE DATASETS & DATALOADERS
# =============================================================================

print("\n" + "="*80)
print("CREATING DATASETS & DATALOADERS")
print("="*80)

# Táº¡o PyTorch Datasets
# Train dataset: Ã¡p dá»¥ng SpecAugment Ä‘á»ƒ tÄƒng tÃ­nh Ä‘a dáº¡ng
train_dataset = AudioDataset(X_train, y_train, apply_specaugment=True)
# Val/Test datasets: khÃ´ng augment Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ chÃ­nh xÃ¡c
val_dataset = AudioDataset(X_val, y_val, apply_specaugment=False)
test_dataset = AudioDataset(X_test, y_test, apply_specaugment=False)

# Táº¡o DataLoaders Ä‘á»ƒ load data theo batch
# Train: shuffle=True Ä‘á»ƒ trÃ¡nh model há»c theo thá»© tá»±
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
# Val/Test: shuffle=False Ä‘á»ƒ káº¿t quáº£ Ä‘á»“ng nháº¥t
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

print(f"âœ“ Train: {len(train_loader)} batches ({len(train_dataset)} samples)")
print(f"âœ“ Val:   {len(val_loader)} batches ({len(val_dataset)} samples)")
print(f"âœ“ Test:  {len(test_loader)} batches ({len(test_dataset)} samples)")
print(f"âœ“ SpecAugment: ENABLED for training")

# =============================================================================
# BUILD MODEL
# =============================================================================

print("\n" + "="*80)
print("BUILDING RESNET18 MODEL")
print("="*80)

# Khá»Ÿi táº¡o model ResNet18 vá»›i pretrained weights tá»« ImageNet
# Chuyá»ƒn model sang GPU/CPU tÃ¹y theo device cÃ³ sáºµn
model = AudioResNet18(num_classes=50, pretrained=True, dropout_rate=0.5).to(DEVICE)

# HÃ m Ä‘áº¿m sá»‘ lÆ°á»£ng parameters trong model
def count_parameters(model):
    """
    Äáº¿m tá»•ng sá»‘ parameters vÃ  sá»‘ parameters trainable
    
    Returns:
        total: Tá»•ng sá»‘ parameters (bao gá»“m cáº£ frozen)
        trainable: Sá»‘ parameters cÃ³ thá»ƒ train (requires_grad=True)
    """
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total, trainable

total_params, trainable_params = count_parameters(model)
print(f"\nModel: ResNet18")
print(f"Total parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")

# =============================================================================
# TRAINING FUNCTIONS
# =============================================================================

def train_epoch(model, loader, criterion, optimizer, device):
    """
    Huáº¥n luyá»‡n model trong 1 epoch
    
    Args:
        model: AudioResNet18 model
        loader: DataLoader training data
        criterion: Loss function (CrossEntropyLoss)
        optimizer: Optimizer (Adam)
        device: 'cuda' hoáº·c 'cpu'
    
    Returns:
        avg_loss: Loss trung bÃ¬nh
        accuracy: Äá»™ chÃ­nh xÃ¡c (%)
    """
    model.train()  # Training mode
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(loader, desc='Training')
    for inputs, labels in pbar:
        # Chuyá»ƒn dá»¯ liá»‡u sang device
        inputs, labels = inputs.to(device), labels.to(device)
        
        # Training step
        optimizer.zero_grad()           # Reset gradients
        outputs = model(inputs)         # Forward pass
        loss = criterion(outputs, labels)  # TÃ­nh loss
        loss.backward()                 # Backward pass (tÃ­nh gradients)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients
        optimizer.step()                # Cáº­p nháº­t weights
        
        # Thá»‘ng kÃª
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        # Hiá»ƒn thá»‹ progress
        pbar.set_postfix({'loss': running_loss/len(loader), 'acc': 100.*correct/total})
    
    return running_loss / len(loader), 100. * correct / total

def validate(model, loader, criterion, device):
    """
    ÄÃ¡nh giÃ¡ model trÃªn táº­p validation/test
    
    Args:
        model: AudioResNet18 model
        loader: DataLoader val/test data
        criterion: Loss function
        device: 'cuda' hoáº·c 'cpu'
    
    Returns:
        avg_loss: Loss trung bÃ¬nh
        accuracy: Äá»™ chÃ­nh xÃ¡c (%)
    """
    model.eval()  # Evaluation mode (dropout táº¯t)
    running_loss = 0.0
    correct = 0
    total = 0
    
    # KhÃ´ng tÃ­nh gradients Ä‘á»ƒ tiáº¿t kiá»‡m memory
    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            # Thá»‘ng kÃª
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    return running_loss / len(loader), 100. * correct / total

# =============================================================================
# TWO-STAGE TRAINING
# =============================================================================

print("\n" + "="*80)
print("TRAINING STRATEGY: TWO-STAGE FINE-TUNING")
print("="*80)
print("\nStage 1: Train only FC layer (backbone frozen) - 20 epochs")
print("Stage 2: Fine-tune entire network - 60 epochs")

# Loss function: CrossEntropyLoss cho multi-class classification
criterion = nn.CrossEntropyLoss()

# =============================================================================
# STAGE 1: TRAIN FC LAYER ONLY
# =============================================================================
# Chiáº¿n lÆ°á»£c: ÄÃ³ng bÄƒng backbone (giá»¯ nguyÃªn pretrained weights)
#             Chá»‰ train FC layer má»›i Ä‘á»ƒ adapt cho ESC-50
#             GiÃºp trÃ¡nh phÃ¡ há»ng features Ä‘Ã£ há»c tá»« ImageNet

print("\n" + "="*80)
print("STAGE 1: TRAINING FC LAYER (Backbone Frozen)")
print("="*80)

# ÄÃ³ng bÄƒng táº¥t cáº£ layers trá»« FC layer
model.freeze_backbone()

# Optimizer chá»‰ optimize cÃ¡c parameters cÃ³ requires_grad=True (FC layer)
optimizer_stage1 = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 
                               lr=LEARNING_RATE, weight_decay=1e-4)  # L2 regularization

# Learning rate scheduler: Giáº£m LR khi val loss khÃ´ng giáº£m
scheduler_stage1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_stage1, mode='min', 
                                                         factor=0.5, patience=5, verbose=True)

# Khá»Ÿi táº¡o biáº¿n theo dÃµi
best_val_acc_stage1 = 0
patience_counter = 0  # Äáº¿m sá»‘ epochs khÃ´ng cáº£i thiá»‡n (cho early stopping)
stage1_epochs = 20

# LÆ°u lá»‹ch sá»­ training
train_losses_s1 = []
train_accs_s1 = []
val_losses_s1 = []
val_accs_s1 = []

# Training loop cho Stage 1
for epoch in range(stage1_epochs):
    print(f"\n[Stage 1] Epoch {epoch+1}/{stage1_epochs}")
    print("-" * 80)
    
    # Train 1 epoch
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer_stage1, DEVICE)
    # Validate
    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)
    
    # Cáº­p nháº­t learning rate náº¿u val loss khÃ´ng giáº£m
    scheduler_stage1.step(val_loss)
    
    # LÆ°u lá»‹ch sá»­
    train_losses_s1.append(train_loss)
    train_accs_s1.append(train_acc)
    val_losses_s1.append(val_loss)
    val_accs_s1.append(val_acc)
    
    print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
    print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")
    
    # LÆ°u model náº¿u val accuracy cáº£i thiá»‡n
    if val_acc > best_val_acc_stage1:
        best_val_acc_stage1 = val_acc
        torch.save(model.state_dict(), 'best_resnet_stage1.pth')
        print(f"âœ“ Stage 1 model saved! (Val Acc: {val_acc:.2f}%)")
        patience_counter = 0
    else:
        patience_counter += 1
    
    # Early stopping: Dá»«ng náº¿u khÃ´ng cáº£i thiá»‡n sau 10 epochs
    if patience_counter >= 10:
        print(f"\nEarly stopping triggered in Stage 1 after {epoch+1} epochs")
        break

print(f"\n=> Stage 1 completed!")
print(f"   Best validation accuracy: {best_val_acc_stage1:.2f}%")

# =============================================================================
# STAGE 2: FINE-TUNE ENTIRE NETWORK
# =============================================================================
# Chiáº¿n lÆ°á»£c: Má»Ÿ bÄƒng toÃ n bá»™ model (backbone + FC)
#             Train vá»›i learning rate tháº¥p hÆ¡n Ä‘á»ƒ khÃ´ng phÃ¡ há»ng pretrained weights
#             Fine-tune toÃ n bá»™ Ä‘á»ƒ adapt tá»‘t hÆ¡n cho audio classification

print("\n" + "="*80)
print("STAGE 2: FINE-TUNING ENTIRE NETWORK")
print("="*80)

# Load model tá»‘t nháº¥t tá»« Stage 1
model.load_state_dict(torch.load('best_resnet_stage1.pth'))
# Má»Ÿ bÄƒng toÃ n bá»™ backbone Ä‘á»ƒ train Ä‘Æ°á»£c
model.unfreeze_backbone()

# Learning rate tháº¥p hÆ¡n (1/10) Ä‘á»ƒ fine-tune cáº©n tháº­n, trÃ¡nh phÃ¡ há»ng weights
optimizer_stage2 = optim.Adam(model.parameters(), lr=LEARNING_RATE/10, weight_decay=1e-4)
# Patience cao hÆ¡n (7) vÃ¬ fine-tune cáº§n thá»i gian
scheduler_stage2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_stage2, mode='min', 
                                                         factor=0.5, patience=7, verbose=True)

# Khá»Ÿi táº¡o tá»« káº¿t quáº£ Stage 1
best_val_acc_stage2 = best_val_acc_stage1
patience_counter = 0
stage2_epochs = 60

# LÆ°u lá»‹ch sá»­ training Stage 2
train_losses_s2 = []
train_accs_s2 = []
val_losses_s2 = []
val_accs_s2 = []

# Training loop cho Stage 2
for epoch in range(stage2_epochs):
    print(f"\n[Stage 2] Epoch {epoch+1}/{stage2_epochs}")
    print("-" * 80)
    
    # Train toÃ n bá»™ model
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer_stage2, DEVICE)
    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)
    
    # Cáº­p nháº­t learning rate
    scheduler_stage2.step(val_loss)
    
    # LÆ°u lá»‹ch sá»­
    train_losses_s2.append(train_loss)
    train_accs_s2.append(train_acc)
    val_losses_s2.append(val_loss)
    val_accs_s2.append(val_acc)
    
    print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
    print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")
    
    # LÆ°u model náº¿u val accuracy cáº£i thiá»‡n
    if val_acc > best_val_acc_stage2:
        best_val_acc_stage2 = val_acc
        torch.save(model.state_dict(), 'best_resnet18_model.pth')
        print(f"âœ“ Stage 2 model saved! (Val Acc: {val_acc:.2f}%)")
        patience_counter = 0
    else:
        patience_counter += 1
    
    # Early stopping vá»›i patience=15
    if patience_counter >= EARLY_STOPPING_PATIENCE:
        print(f"\nEarly stopping triggered in Stage 2 after {epoch+1} epochs")
        break

print(f"\n=> Stage 2 completed!")
print(f"   Best validation accuracy: {best_val_acc_stage2:.2f}%")

# Káº¿t há»£p lá»‹ch sá»­ training tá»« cáº£ 2 stages
train_losses = train_losses_s1 + train_losses_s2
train_accs = train_accs_s1 + train_accs_s2
val_losses = val_losses_s1 + val_losses_s2
val_accs = val_accs_s1 + val_accs_s2

# =============================================================================
# EVALUATION
# =============================================================================

print("\n" + "="*80)
print("EVALUATING ON TEST SET")
print("="*80)

# Load model tá»‘t nháº¥t (tá»« Stage 2)
model.load_state_dict(torch.load('best_resnet18_model.pth'))
model.eval()  # Chuyá»ƒn sang evaluation mode

# Dá»± Ä‘oÃ¡n trÃªn táº­p test
all_preds = []
all_labels = []

with torch.no_grad():  # KhÃ´ng tÃ­nh gradients Ä‘á»ƒ tiáº¿t kiá»‡m memory
    for inputs, labels in tqdm(test_loader, desc='Testing'):
        inputs = inputs.to(DEVICE)
        outputs = model(inputs)  # Forward pass
        _, predicted = outputs.max(1)  # Láº¥y class cÃ³ xÃ¡c suáº¥t cao nháº¥t
        
        # LÆ°u káº¿t quáº£
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.numpy())

# Chuyá»ƒn sang numpy arrays
y_pred = np.array(all_preds)
y_true = np.array(all_labels)

# TÃ­nh accuracy trÃªn táº­p test
accuracy = accuracy_score(y_true, y_pred)
print(f"\n=> TEST ACCURACY: {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"   Correct: {np.sum(y_pred == y_true)}/{len(y_true)}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_true, y_pred))

# Váº½ Confusion Matrix Ä‘á»ƒ xem model nháº§m láº«n giá»¯a cÃ¡c classes nÃ o
plt.figure(figsize=(12, 10))
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', cbar=True)
plt.title(f'Confusion Matrix - ResNet18 (Accuracy: {accuracy:.2%})')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig('confusion_matrix_ResNet18.png', dpi=300)
plt.close()
print("\n=> Saved: confusion_matrix_ResNet18.png")

# =============================================================================
# TRAINING HISTORY
# =============================================================================

print("\n" + "="*80)
print("TRAINING HISTORY")
print("="*80)

plt.figure(figsize=(16, 5))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(train_accs, label='Train Accuracy', linewidth=2)
plt.plot(val_accs, label='Val Accuracy', linewidth=2)
plt.axvline(x=len(train_accs_s1), color='r', linestyle='--', label='Stage 1 â†’ Stage 2')
plt.title('ResNet18 Training: Accuracy', fontsize=14, fontweight='bold')
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Accuracy (%)', fontsize=12)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(train_losses, label='Train Loss', linewidth=2)
plt.plot(val_losses, label='Val Loss', linewidth=2)
plt.axvline(x=len(train_losses_s1), color='r', linestyle='--', label='Stage 1 â†’ Stage 2')
plt.title('ResNet18 Training: Loss', fontsize=14, fontweight='bold')
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_history_ResNet18.png', dpi=300)
plt.close()
print("=> Saved: training_history_ResNet18.png")

# Best epoch
best_epoch_idx = np.argmax(val_accs)
print(f"\nBest Epoch: {best_epoch_idx + 1}")
print(f"Best Val Accuracy: {val_accs[best_epoch_idx]:.2f}%")

# =============================================================================
# MODEL COMPARISON
# =============================================================================
# So sÃ¡nh ResNet18 vá»›i cÃ¡c approaches khÃ¡c:
# - SVM (Traditional ML): Handcrafted features
# - CNN (Custom): Deep learning tá»« Ä‘áº§u
# - ResNet18: Transfer learning

print("\n" + "="*80)
print("MODEL COMPARISON")
print("="*80)

comparison = {
    'Model': ['SVM', 'CNN (Custom)', 'ResNet18 (Transfer Learning)'],
    'Accuracy': [0.7625, 0.8025, accuracy],
    'Parameters': ['~200 features', '1.28M', '11.2M'],
    'Training': ['2 min', '~100 epochs', f'{len(train_accs)} epochs (2-stage)']
}

comparison_df = pd.DataFrame(comparison)
print(comparison_df.to_string(index=False))

# TÃ­nh má»©c Ä‘á»™ cáº£i thiá»‡n so vá»›i cÃ¡c baseline
improvement_vs_svm = (accuracy - 0.7625) / 0.7625 * 100
improvement_vs_cnn = (accuracy - 0.8025) / 0.8025 * 100

print(f"\n=> Improvement:")
print(f"   vs SVM: +{improvement_vs_svm:.2f}% ({0.7625*100:.2f}% â†’ {accuracy*100:.2f}%)")
print(f"   vs CNN: +{improvement_vs_cnn:.2f}% ({0.8025*100:.2f}% â†’ {accuracy*100:.2f}%)")

# ÄÃ¡nh giÃ¡ káº¿t quáº£
if accuracy >= 0.85:
    print(f"\nðŸŽ‰ Má»¤C TIÃŠU Äáº T ÄÆ¯á»¢C: 85%+!")
    if accuracy >= 0.90:
        print(f"ðŸ† EXCELLENT: 90%+!")
elif accuracy >= 0.80:
    print(f"\nâœ… Tá»‘t! ÄÃ£ vÆ°á»£t CNN baseline!")
else:
    print(f"\nâš ï¸ Cáº§n thÃªm tuning")

# =============================================================================
# SAVE MODEL INFO
# =============================================================================

model_info = f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RESNET18 TRANSFER LEARNING FOR AUDIO CLASSIFICATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DATASET:
  - ESC-50: {len(y_train) + len(y_val) + len(y_test)} samples, 50 classes
  - Train: {len(y_train)} samples (augmented)
  - Val: {len(y_val)} samples
  - Test: {len(y_test)} samples
  - SpecAugment: ENABLED for training

MODEL:
  - Architecture: ResNet18 (pretrained on ImageNet)
  - Total parameters: {total_params:,}
  - Trainable parameters: {trainable_params:,}
  - Input: Mel Spectrogram (128x128) converted to 3-channel RGB
  - Dropout: 0.5

TRAINING STRATEGY:
  - Stage 1: FC layer only ({len(train_accs_s1)} epochs)
    Best Val Acc: {best_val_acc_stage1:.2f}%
  
  - Stage 2: Full fine-tuning ({len(train_accs_s2)} epochs)
    Best Val Acc: {best_val_acc_stage2:.2f}%
  
  - Total epochs: {len(train_accs)}
  - Optimizer: Adam (Stage 1: lr=0.001, Stage 2: lr=0.0001)
  - Scheduler: ReduceLROnPlateau
  - Early stopping: patience={EARLY_STOPPING_PATIENCE}

RESULTS:
  - Train Accuracy: {train_accs[best_epoch_idx]:.2f}%
  - Val Accuracy: {best_val_acc_stage2:.2f}%
  - Test Accuracy: {accuracy*100:.2f}%
  - Improvement vs SVM: +{improvement_vs_svm:.2f}%
  - Improvement vs CNN: +{improvement_vs_cnn:.2f}%

FILES:
  âœ“ best_resnet18_model.pth
  âœ“ confusion_matrix_ResNet18.png
  âœ“ training_history_ResNet18.png
  âœ“ resnet_model_info.txt

USAGE:
  from resnet_model import AudioResNet18
  import torch
  
  model = AudioResNet18(num_classes=50, pretrained=False)
  model.load_state_dict(torch.load('best_resnet18_model.pth'))
  model.eval()

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

with open('resnet_model_info.txt', 'w', encoding='utf-8') as f:
    f.write(model_info)

print(model_info)
print("=> Saved: resnet_model_info.txt")

print("\n" + "="*80)
print("ðŸš€ TRAINING COMPLETED!")
print("="*80)
print("\nGenerated files:")
print("  âœ“ best_resnet18_model.pth")
print("  âœ“ best_resnet_stage1.pth")
print("  âœ“ confusion_matrix_ResNet18.png")
print("  âœ“ training_history_ResNet18.png")
print("  âœ“ resnet_model_info.txt")
print("="*80)

